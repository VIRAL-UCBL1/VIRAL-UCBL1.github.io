{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to VIRAL","text":""},{"location":"#viral","title":"VIRAL","text":"<p>Vision-grounded Integration for Reward design And Learning.</p>"},{"location":"#schema","title":"Schema","text":""},{"location":"setup/","title":"Welcome to VIRAL Installation Guide","text":""},{"location":"setup/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11</li> <li>Ollama.</li> <li>PyTorch.</li> <li>CUDA</li> <li>Git.</li> <li>Conda.</li> </ul> <pre><code>git clone https://github.com/VIRAL-UCBL1/VIRAL.git\ncd VIRAL\n</code></pre>"},{"location":"setup/#installation","title":"Installation","text":"<p>Before installing the dependencies, create a virtual environment with conda or venv.</p> <ul> <li>Python requirements:</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ollama:</li> </ul> <pre><code>ollama run qwen2.5-coder\n</code></pre>"},{"location":"setup/#usage","title":"Usage","text":"<pre><code>cd src\npython main.py\n</code></pre> <p>You can see help message by running:</p> <pre><code>python main.py -h\n</code></pre>"},{"location":"code_docs/ObjectivesMetrics/","title":"ObjectivesMetrics","text":""},{"location":"code_docs/ObjectivesMetrics/#src.utils.ObjectivesMetrics.objective_metric_CartPole","title":"<code>objective_metric_CartPole(states)</code>","text":"<p>Objective metric for the CartPole environment. Calculates a score for the given state on a particular observation of the CartPole environment.</p> <p>:param state: The state of the CartPole environment. :return: a table of tuples containing the string name of the metric and the value of the metric.</p>"},{"location":"code_docs/OllamaChat/","title":"OllamaChat","text":""},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat","title":"<code>OllamaChat</code>","text":""},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.__init__","title":"<code>__init__(model='qwen2.5-coder', system_prompt=None, options=None)</code>","text":"<p>Initialize an advanced Ollama chat session with extended configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the Ollama model.</p> <code>'qwen2.5-coder'</code> <code>system_prompt</code> <code>str</code> <p>Initial system message to set chat context.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Advanced model generation parameters.</p> <code>None</code>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.add_message","title":"<code>add_message(content, role='user', **kwargs)</code>","text":"<p>Add a message to the chat history with optional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content</p> required <code>role</code> <code>str</code> <p>Message role (user/assistant/system)</p> <code>'user'</code>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.generate_response","title":"<code>generate_response(stream=False, additional_options=None)</code>","text":"<p>Generate a response with advanced configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>Stream response in real-time</p> <code>False</code> <code>additional_options</code> <code>dict</code> <p>Temporary generation options</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, Generator]</code> <p>Response as string or streaming generator</p>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.print_Generator_and_return","title":"<code>print_Generator_and_return(response, number=1)</code>","text":"<p>Prints the content of a response if it is a generator, or simply returns the response as is.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Generator | str</code> <p>The response to print or return. If it's a generator,                          it will be printed chunk by chunk. If it's a string,                          it will be returned directly.</p> required <code>number</code> <code>int</code> <p>The index of the response (default is 1). Used for logging purposes.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The original response if it is a string, or the concatenated string of all chunks </p> <code>str</code> <p>if it was a generator.</p>"},{"location":"code_docs/State/","title":"State","text":""},{"location":"code_docs/State/#src.utils.State.State","title":"<code>State</code>","text":""},{"location":"code_docs/State/#src.utils.State.State.__init__","title":"<code>__init__(idx, reward_func=None, reward_func_str=None, policy=None, perfomances=None)</code>","text":"<p>Represents a state in the reward function generation and evaluation process.</p> <p>This class encapsulates the key components of a reward function's lifecycle, tracking its index, implementation, policy, and performance metrics.</p> <p>Attributes:</p> Name Type Description <code>idx</code> <code>int</code> <p>Unique identifier for the state.</p> <code>reward_func</code> <code>Callable</code> <p>The compiled reward function.</p> <code>reward_func_str</code> <code>str</code> <p>String representation of the reward function.</p> <code>policy</code> <code>object</code> <p>The policy associated with the reward function.</p> <code>performances</code> <code>dict</code> <p>Performance metrics of the reward function.</p> Key Characteristics <ul> <li>Tracks the evolution of reward functions</li> <li>Provides a snapshot of a specific iteration</li> <li>Allows for dynamic updating of policy and performance</li> </ul> Initialization Constraints <ul> <li>Initial state (idx=0) cannot have a reward function</li> <li>Non-initial states must have both a reward function and its string representation</li> </ul> <p>Functions:</p> Name Description <code>- set_policy</code> <p>Update the associated policy</p> <code>- set_performances</code> <p>Update performance metrics</p> <code>- __repr__</code> <p>Provide a human-readable string representation of the state</p> Example Notes <ul> <li>Designed for tracking reward function iterations</li> <li>Provides flexibility in managing function states</li> <li>Supports logging and debugging of reward function generation process</li> </ul>"},{"location":"code_docs/State/#src.utils.State.State.__init__--creating-a-new-state-for-a-reward-function","title":"Creating a new state for a reward function","text":"<p>state = State(     idx=1,      reward_func=my_reward_func,      reward_func_str=\"def reward_func(...):\",     policy=None,     perfomances=None )</p>"},{"location":"code_docs/State/#src.utils.State.State.__init__--updating-state-with-training-results","title":"Updating state with training results","text":"<p>state.set_policy(trained_policy) state.set_performances({     'success_rate': 0.75,     'average_reward': 10.5 })</p>"},{"location":"code_docs/VIRAL/","title":"Overview of VIRAL","text":""},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL","title":"<code>VIRAL</code>","text":""},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.__init__","title":"<code>__init__(learning_algo, env_type, success_function, objectives_metrics=[], model='qwen2.5-coder', options={})</code>","text":"<p>Initialize VIRAL architecture for dynamic reward function generation     Args:         model (str): Language model for reward generation         learning_method (str): Reinforcement learning method</p>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.compile_reward_function","title":"<code>compile_reward_function(response)</code>","text":"<p>Compile a reward function dynamically from a string response.</p> <p>This method takes a code string representing a reward function and dynamically  compiles it into an executable Python function. It provides a secure way to  generate reward functions for reinforcement learning environments.</p> Key Features <ul> <li>Dynamically executes code in an isolated global namespace</li> <li>Provides access to NumPy functions</li> <li>Extracts the compiled function by its name</li> <li>Robust error handling for syntax issues</li> </ul> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>A string containing a complete Python function definition              for a reward function.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The compiled reward function that can be called with appropriate      arguments in a gym environment.</p> <p>Raises:</p> Type Description <code>SyntaxError</code> <p>If the provided code contains invalid Python syntax.</p> <code>ValueError</code> <p>If the function cannot be extracted from the compiled namespace.</p> Notes <ul> <li>Uses <code>exec()</code> for dynamic code compilation</li> <li>Provides NumPy (<code>np</code>) in the execution namespace</li> <li>Assumes the last function defined in the response is the reward function</li> </ul>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.evaluate_policy","title":"<code>evaluate_policy(idx1, idx2)</code>","text":"<p>Evaluate policy performance for multiple reward functions</p> <p>Parameters:</p> Name Type Description Default <code>objectives_metrics</code> <code>List[callable]</code> <p>Custom objective metrics</p> required <code>num_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>int</code> <p>Performance metrics for multiple reward functions</p>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.generate_reward_function","title":"<code>generate_reward_function(task_description, iterations=1)</code>","text":"<p>Generate and iteratively improve a reward function using a Language Model (LLM).</p> <p>This method implements a sophisticated reward function generation process  that involves multiple stages of creation, evaluation, and refinement.</p> Key Stages <ol> <li>Initial Function Generation: Create two initial reward function candidates</li> <li>Evaluation: Compare and identify the best and worst performing functions</li> <li>Iterative Refinement: Progressively improve the worst-performing function</li> </ol> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>A detailed description of the task or environment                      for which the reward function is being generated.</p> required <code>iterations</code> <code>int</code> <p>Number of refinement iterations to perform.                          Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>List[State]</code> <p>List[State]: A list of generated and refined reward function states,          containing information about each function's performance          and implementation.</p> Process Overview <ul> <li>Generates two initial reward functions using an LLM</li> <li>Evaluates these functions using a policy evaluation method</li> <li>Selects the worst-performing function for refinement</li> <li>Iteratively refines the function through self-refinement</li> <li>Tracks the evolution of reward functions in the memory</li> </ul> Detailed Workflow <ol> <li>Generate two initial reward functions<ul> <li>Uses a predefined prompt template</li> <li>Applies configurable LLM generation options</li> <li>Compiles and tests each generated function</li> </ul> </li> <li>Evaluates initial functions<ul> <li>Identifies best and worst performing functions</li> </ul> </li> <li>Iterative Refinement<ul> <li>Applies self-refinement to the worst-performing function</li> <li>Re-evaluates after each refinement</li> <li>Repeats for specified number of iterations</li> </ul> </li> </ol> Note <ul> <li>Uses dynamic LLM configuration options</li> <li>Supports flexible environment types</li> <li>Provides a systematic approach to reward function generation</li> <li>Logging at various stages for debugging and tracking</li> </ul>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.get_code","title":"<code>get_code(response)</code>","text":"<p>Clean and validate a code response by removing code block markers and ensuring a function definition.</p> <p>This method is designed to process code responses, typically extracted from text or code blocks, by performing the following operations:</p> <ol> <li>Remove leading and trailing code block markers (```),</li> <li>Remove the 'python' language identifier,</li> <li>Strip any additional whitespace</li> <li>Validate that the response contains a function definition</li> </ol> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The raw code response to be cleaned and validated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned code response containing a function definition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the response does not contain a valid function definition          (i.e., if \"def \" is not present in the cleaned response).</p> Logging <p>Logs the cleaned code at DEBUG level for debugging purposes.</p>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.get_runnable_function","title":"<code>get_runnable_function(response, error=None)</code>","text":"<p>Process and validate a reward function for a gym environment.</p> <p>This method attempts to generate and validate a reward function by:</p> <ol> <li>Handling potential previous errors</li> <li>Creating a gym environment</li> <li>Cleaning and compiling the code</li> <li>Testing the reward function with a sample action</li> <li>Recursively handling various potential errors</li> </ol> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The code response containing the reward function definition.</p> required <code>error</code> <code>str</code> <p>Previous error message to be added to LLM context.                      Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Callable</code> <p>A tuple containing: - Callable: The compiled and validated reward function - str: The original response code</p> <p>Raises:</p> Type Description <code>-ValueError</code> <p>Invalid function definition</p> <code>-SyntaxError</code> <p>Syntax issues in the function</p> <code>-RuntimeError</code> <p>Execution problems during function testing</p> Note <ul> <li>Uses recursion to handle potential errors</li> <li>Relies on get_code, compile_reward_function, and test_reward_function methods</li> <li>Provides a robust mechanism for generating valid reward functions</li> </ul>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.self_refine_reward","title":"<code>self_refine_reward(idx)</code>","text":"<p>Iteratively improve a reward function using self-refinement techniques.</p> <p>This method implements an intelligent self-refinement process for reward functions by leveraging a Language Model (LLM) to analyze and improve the current function based on its previous performance.</p> Key Objectives <ul> <li>Analyze current reward function performance</li> <li>Generate an improved version of the reward function</li> <li>Maintain the core task objectives while optimizing the reward signal</li> </ul> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the reward function in the memory to be refined.     Typically the worst-performing function from previous evaluation.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>Callable</code> <p>Index of the newly created refined reward function in the memory.</p> Refinement Process <ol> <li>Construct a refinement prompt with:<ul> <li>Current reward function code</li> <li>Performance metrics</li> <li>Explicit refinement goals</li> </ul> </li> <li>Generate a new reward function using LLM</li> <li>Compile and validate the new function</li> <li>Append the new function to memory</li> <li>Return the index of the new function</li> </ol> Refinement Goals <ul> <li>Increase success rate of the policy</li> <li>Optimize the reward signal for better learning</li> <li>Preserve the original task objectives</li> <li>Improve overall performance</li> </ul> Notes <ul> <li>Uses the existing memory to track function evolution</li> <li>Leverages LLM for intelligent function refinement</li> <li>Provides a systematic approach to reward function improvement</li> <li>Maintains a history of function iterations</li> </ul>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.test_reward_function","title":"<code>test_reward_function(reward_function, *args, **kwargs)</code>","text":"<p>Test the compiled reward function with provided inputs to validate its execution.</p> <p>This method serves as a crucial validation step in the reward function generation  process. It attempts to execute the reward function with the given arguments and  logs the output or raises an error if execution fails.</p> Purpose <ul> <li>Verify the reward function can be executed without errors</li> <li>Log the reward function's output for debugging</li> <li>Ensure the function returns a valid result in the context of a gym environment</li> </ul> <p>Parameters:</p> Name Type Description Default <code>reward_function</code> <code>Callable</code> <p>The compiled reward function to be tested.</p> required <code>*args</code> <p>Variable length argument list to pass to the reward function. Typically includes observations, actions, or environment states.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments to pass to the reward function. May include additional context like 'terminated' or 'truncated' flags.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the reward function fails to execute successfully. This includes any exceptions that occur during function invocation.</p> Logging <ul> <li>Logs the reward function's output at DEBUG level when successful</li> <li>Provides detailed error information if execution fails</li> </ul> Notes <ul> <li>Designed to be flexible with varying function signatures</li> <li>Critical for validating dynamically generated reward functions</li> <li>Part of the reward function generation quality control process</li> </ul>"},{"location":"code_docs/main/","title":"Main","text":""},{"location":"code_docs/main/#src.main.main","title":"<code>main()</code>","text":"<p>Main entry point of the script.</p> <p>This block is executed when the script is run directly. It initializes the logger, and run VIRAL. It uses CLI interface. memory.</p>"},{"location":"code_docs/main/#src.main.parse_logger","title":"<code>parse_logger()</code>","text":"<p>Parses command-line arguments to configure the logger.</p> <p>Returns:</p> Name Type Description <code>Logger</code> <p>Configured logger instance.</p>"},{"location":"code_docs/utils/","title":"Utils","text":""},{"location":"code_docs/utils/#src.utils.utils.plot_sumrwdperepi","title":"<code>plot_sumrwdperepi(sum_rewards)</code>","text":"<p>trace courbe de somme des rec par episodes</p>"},{"location":"code_docs/utils/#src.utils.utils.plot_sumrwdperepi_movingavg","title":"<code>plot_sumrwdperepi_movingavg(sum_rewards, avgs)</code>","text":"<p>trace courbe de somme des rec (sum_rewards) et moyenne glissante (avgs) par episodes</p>"},{"location":"code_docs/utils/#src.utils.utils.plot_sumrwdperepi_overseed2","title":"<code>plot_sumrwdperepi_overseed2(rewards_over_seeds)</code>","text":"<p>trace courbe de somme des rec par episodes moyenne + std sur plusieurs seeds</p>"},{"location":"code_docs/RLAlgo/DirectSearch/","title":"DirectSearch","text":""},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch","title":"<code>DirectSearch</code>","text":""},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.__init__","title":"<code>__init__(env, nb_episodes=2000, max_t=1000, det=True)</code>","text":"<p>env: Environnement Gymnasium det: Si la politique est d\u00e9terministe ou stochastique</p>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.output","title":"<code>output(etat)</code>","text":"<p>Calcul de la sortie de la politique - si d\u00e9terministe : argmax - si stochastique : probabilit\u00e9 de chaque action</p>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.rollout","title":"<code>rollout(reward_func)</code>","text":"<p>execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode</p>"},{"location":"code_docs/RLAlgo/PPO/","title":"PPO","text":""},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO","title":"<code>PPO</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.__init__","title":"<code>__init__(env, hidden_size=256, std=0.0, batch_size=1, ppo_epoch=4, lr=0.0003, nb_episodes=2000, max_t=1000)</code>","text":""},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.load","title":"<code>load(file)</code>","text":"<p>load the model form file</p>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.output","title":"<code>output(state)</code>","text":""},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.save","title":"<code>save(file)</code>","text":"<p>save the model.</p>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.learning","title":"<code>learning(learning_method, env)</code>","text":"<p>train a policy on an environment</p>"},{"location":"code_docs/RLAlgo/Reinforce/","title":"Reinforce","text":""},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce","title":"<code>Reinforce</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.__init__","title":"<code>__init__(env, couche_cachee, nb_episodes=2000, max_t=1000, gamma=0.99)</code>","text":"<p>env: Environnement Gymnasium couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32]) gamma: Facteur de discount pour le calcul des retours cumul\u00e9s det: Si la politique est d\u00e9terministe ou stochastique</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.action","title":"<code>action(etat)</code>","text":"<p>Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action. etat: un \u00e9tat de l'environnement return: action \u00e0 ex\u00e9cuter et la log-proba de cette action</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.calcul_retours_cumules","title":"<code>calcul_retours_cumules(recompenses)</code>","text":"<p>Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma. recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire return: liste des retours cumul\u00e9s</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.forward","title":"<code>forward(etat)</code>","text":"<p>Etat: un tenseur d'\u00e9tat (entr\u00e9e) return: Distribution de probabilit\u00e9 sur les actions</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.load","title":"<code>load(file)</code>","text":"<p>load the model form file</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.loss","title":"<code>loss(log_probs, retours_cumules)</code>","text":"<p>Calcule la perte de type REINFORCE. log_probs: liste des log-probas des actions prises lors d'une trajectoire retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s return: perte</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.output","title":"<code>output(etat)</code>","text":"<p>Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.save","title":"<code>save(file)</code>","text":"<p>save the model.</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.train","title":"<code>train(reward_func=None, save_name='', stop=None)</code>","text":"<p>Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.</p> <p>Parameters:</p> Name Type Description Default <code>reward_func</code> <code>callable</code> <p>Fonction de r\u00e9compense personnalis\u00e9e.</p> <code>None</code> <code>nb_episodes</code> <code>int</code> <p>Nombre maximum d'\u00e9pisodes.</p> required <code>max_t</code> <code>int</code> <p>Nombre maximum de pas par \u00e9pisode.</p> required <code>save_name</code> <code>str</code> <p>Chemin pour sauvegarder le mod\u00e8le.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict, list, float, int]</code> <p>(Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).</p>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.trajectoire","title":"<code>trajectoire(reward_fonc)</code>","text":"<p>Simule une trajectoire dans l'environnement en utilisant la politique. env: Environnement Gymnasium max_t: nombre max de pas de la trajectoire return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e</p>"}]}