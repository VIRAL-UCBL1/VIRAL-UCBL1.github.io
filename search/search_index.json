{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to VIRAL","text":""},{"location":"#viral","title":"VIRAL","text":"<p>Vision-grounded Integration for Reward design And Learning.</p>"},{"location":"#schema","title":"Schema","text":""},{"location":"setup/","title":"Welcome to VIRAL Installation Guide","text":""},{"location":"setup/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11</li> <li>Ollama.</li> <li>PyTorch.</li> <li>CUDA</li> <li>Git.</li> <li>Conda.</li> </ul> <pre><code>git clone https://github.com/VIRAL-UCBL1/VIRAL.git\ncd VIRAL\n</code></pre>"},{"location":"setup/#installation","title":"Installation","text":"<p>Before installing the dependencies, create a virtual environment with conda or venv.</p> <ul> <li>Python requirements:</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ollama:</li> </ul> <pre><code>ollama run qwen2.5-coder\n</code></pre>"},{"location":"setup/#usage","title":"Usage","text":"<pre><code>cd src\npython main.py\n</code></pre> <p>You can see help message by running:</p> <pre><code>python main.py -h\n</code></pre>"},{"location":"code_docs/ObjectivesMetrics/","title":"ObjectivesMetrics","text":""},{"location":"code_docs/ObjectivesMetrics/#src.utils.ObjectivesMetrics.objective_metric_CartPole","title":"<code>objective_metric_CartPole(states)</code>","text":"<p>Objective metric for the CartPole environment. Calculates a score for the given state on a particular observation of the CartPole environment.</p> <p>:param state: The state of the CartPole environment. :return: a table of tuples containing the string name of the metric and the value of the metric.</p> Source code in <code>src/utils/ObjectivesMetrics.py</code> <pre><code>def objective_metric_CartPole(states):\n    \"\"\"\n    Objective metric for the CartPole environment.\n    Calculates a score for the given state on a particular observation of the CartPole environment.\n\n    :param state: The state of the CartPole environment.\n    :return: a table of tuples containing the string name of the metric and the value of the metric.\n    \"\"\"\n\n    # Calculate the difference between the pole angle and the median of the pole angle range\n    pole_angle_diff = 0\n    for state in states:\n        pole_angle = state[2]\n        pole_angle_diff += abs(pole_angle)\n    pole_angle_diff = pole_angle_diff / len(states)\n\n    # Calculate the difference between the pole position and the median of the pole position range\n    pole_position_diff = 0\n    for state in states:\n        pole_position = state[0]\n        pole_position_diff += abs(pole_position)\n    pole_position_diff = pole_position_diff / len(states)\n\n    result = [\n        {\"pole_angle_diff\": pole_angle_diff},\n        {\"pole_position_diff\": pole_position_diff},\n    ]\n\n    return result\n</code></pre>"},{"location":"code_docs/OllamaChat/","title":"OllamaChat","text":""},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat","title":"<code>OllamaChat</code>","text":"Source code in <code>src/utils/OllamaChat.py</code> <pre><code>class OllamaChat:\n    def __init__(\n        self,\n        model: str = \"qwen2.5-coder\",\n        system_prompt: Optional[str] = None,\n        options: Optional[Dict] = None,\n    ):\n        \"\"\"\n        Initialize an advanced Ollama chat session with extended configuration.\n\n        Args:\n            model (str, optional): The name of the Ollama model.\n            system_prompt (str, optional): Initial system message to set chat context.\n            options (dict, optional): Advanced model generation parameters.\n        \"\"\"\n        self.model = model\n        self.messages: List[Dict[str, str]] = []\n        self.options = options or {}\n\n        self.logger = getLogger(\"VIRAL\")\n\n        if system_prompt:\n            self.logger.info(f\"System: {system_prompt}, Options: {self.options}\")\n            self.add_message(system_prompt, role=\"system\")\n\n    def add_message(self, content: str, role: str = \"user\", **kwargs) -&gt; None:\n        \"\"\"\n        Add a message to the chat history with optional metadata.\n\n        Args:\n            content (str): The message content\n            role (str, optional): Message role (user/assistant/system)\n        \"\"\"\n        message = {\"role\": role, \"content\": content, **kwargs}\n        self.messages.append(message)\n\n    def generate_response(\n        self, stream: bool = False, additional_options: Optional[Dict] = None\n    ) -&gt; Union[str, Generator]:\n        \"\"\"\n        Generate a response with advanced configuration options.\n\n        Args:\n            stream (bool, optional): Stream response in real-time\n            additional_options (dict, optional): Temporary generation options\n\n        Returns:\n            Response as string or streaming generator\n        \"\"\"\n        generation_options = {**self.options, **(additional_options or {})}\n\n        payload = {\n            \"model\": self.model,\n            \"messages\": self.messages,\n            \"stream\": stream,\n            \"options\": generation_options,\n        }\n\n        try:\n            response = requests.post(OLLAMA_CHAT_API_URL, json=payload, stream=stream)\n\n            response.raise_for_status()\n            if not stream:\n                full_response = response.json()\n                assistant_response = full_response.get(\"message\", {}).get(\"content\", \"\")\n                self.add_message(assistant_response, role=\"assistant\")\n                return assistant_response\n\n            def stream_response():\n                full_response = \"\"\n                for line in response.iter_lines():\n                    if line:\n                        try:\n                            json_response = json.loads(line.decode(\"utf-8\"))\n                            if \"message\" in json_response:\n                                chunk = json_response[\"message\"].get(\"content\", \"\")\n                                full_response += chunk\n                                yield chunk\n                        except json.JSONDecodeError:\n                            continue\n\n                if full_response:\n                    self.add_message(full_response, role=\"assistant\")\n\n            return stream_response()\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Connection error: {e}\")\n            return \"\"\n\n    def print_Generator_and_return(self, response: Generator | str, number: int = 1) -&gt; str:\n        \"\"\"\n        Prints the content of a response if it is a generator, or simply returns the response as is.\n\n        Args:\n            response (Generator | str): The response to print or return. If it's a generator, \n                                        it will be printed chunk by chunk. If it's a string, \n                                        it will be returned directly.\n            number (int, optional): The index of the response (default is 1). Used for logging purposes.\n\n        Returns:\n            The original response if it is a string, or the concatenated string of all chunks \n            if it was a generator.\n        \"\"\"\n        self.logger.debug(f\"Response {number}:\")\n        if isinstance(response, Generator):\n            response_gen = response\n            response = \"\"\n            for chunk in response_gen:\n                print(chunk, end=\"\", flush=True)\n                response += chunk\n        return response\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.__init__","title":"<code>__init__(model='qwen2.5-coder', system_prompt=None, options=None)</code>","text":"<p>Initialize an advanced Ollama chat session with extended configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the Ollama model.</p> <code>'qwen2.5-coder'</code> <code>system_prompt</code> <code>str</code> <p>Initial system message to set chat context.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Advanced model generation parameters.</p> <code>None</code> Source code in <code>src/utils/OllamaChat.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"qwen2.5-coder\",\n    system_prompt: Optional[str] = None,\n    options: Optional[Dict] = None,\n):\n    \"\"\"\n    Initialize an advanced Ollama chat session with extended configuration.\n\n    Args:\n        model (str, optional): The name of the Ollama model.\n        system_prompt (str, optional): Initial system message to set chat context.\n        options (dict, optional): Advanced model generation parameters.\n    \"\"\"\n    self.model = model\n    self.messages: List[Dict[str, str]] = []\n    self.options = options or {}\n\n    self.logger = getLogger(\"VIRAL\")\n\n    if system_prompt:\n        self.logger.info(f\"System: {system_prompt}, Options: {self.options}\")\n        self.add_message(system_prompt, role=\"system\")\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.add_message","title":"<code>add_message(content, role='user', **kwargs)</code>","text":"<p>Add a message to the chat history with optional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content</p> required <code>role</code> <code>str</code> <p>Message role (user/assistant/system)</p> <code>'user'</code> Source code in <code>src/utils/OllamaChat.py</code> <pre><code>def add_message(self, content: str, role: str = \"user\", **kwargs) -&gt; None:\n    \"\"\"\n    Add a message to the chat history with optional metadata.\n\n    Args:\n        content (str): The message content\n        role (str, optional): Message role (user/assistant/system)\n    \"\"\"\n    message = {\"role\": role, \"content\": content, **kwargs}\n    self.messages.append(message)\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.generate_response","title":"<code>generate_response(stream=False, additional_options=None)</code>","text":"<p>Generate a response with advanced configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>Stream response in real-time</p> <code>False</code> <code>additional_options</code> <code>dict</code> <p>Temporary generation options</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, Generator]</code> <p>Response as string or streaming generator</p> Source code in <code>src/utils/OllamaChat.py</code> <pre><code>def generate_response(\n    self, stream: bool = False, additional_options: Optional[Dict] = None\n) -&gt; Union[str, Generator]:\n    \"\"\"\n    Generate a response with advanced configuration options.\n\n    Args:\n        stream (bool, optional): Stream response in real-time\n        additional_options (dict, optional): Temporary generation options\n\n    Returns:\n        Response as string or streaming generator\n    \"\"\"\n    generation_options = {**self.options, **(additional_options or {})}\n\n    payload = {\n        \"model\": self.model,\n        \"messages\": self.messages,\n        \"stream\": stream,\n        \"options\": generation_options,\n    }\n\n    try:\n        response = requests.post(OLLAMA_CHAT_API_URL, json=payload, stream=stream)\n\n        response.raise_for_status()\n        if not stream:\n            full_response = response.json()\n            assistant_response = full_response.get(\"message\", {}).get(\"content\", \"\")\n            self.add_message(assistant_response, role=\"assistant\")\n            return assistant_response\n\n        def stream_response():\n            full_response = \"\"\n            for line in response.iter_lines():\n                if line:\n                    try:\n                        json_response = json.loads(line.decode(\"utf-8\"))\n                        if \"message\" in json_response:\n                            chunk = json_response[\"message\"].get(\"content\", \"\")\n                            full_response += chunk\n                            yield chunk\n                    except json.JSONDecodeError:\n                        continue\n\n            if full_response:\n                self.add_message(full_response, role=\"assistant\")\n\n        return stream_response()\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Connection error: {e}\")\n        return \"\"\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.utils.OllamaChat.OllamaChat.print_Generator_and_return","title":"<code>print_Generator_and_return(response, number=1)</code>","text":"<p>Prints the content of a response if it is a generator, or simply returns the response as is.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Generator | str</code> <p>The response to print or return. If it's a generator,                          it will be printed chunk by chunk. If it's a string,                          it will be returned directly.</p> required <code>number</code> <code>int</code> <p>The index of the response (default is 1). Used for logging purposes.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The original response if it is a string, or the concatenated string of all chunks </p> <code>str</code> <p>if it was a generator.</p> Source code in <code>src/utils/OllamaChat.py</code> <pre><code>def print_Generator_and_return(self, response: Generator | str, number: int = 1) -&gt; str:\n    \"\"\"\n    Prints the content of a response if it is a generator, or simply returns the response as is.\n\n    Args:\n        response (Generator | str): The response to print or return. If it's a generator, \n                                    it will be printed chunk by chunk. If it's a string, \n                                    it will be returned directly.\n        number (int, optional): The index of the response (default is 1). Used for logging purposes.\n\n    Returns:\n        The original response if it is a string, or the concatenated string of all chunks \n        if it was a generator.\n    \"\"\"\n    self.logger.debug(f\"Response {number}:\")\n    if isinstance(response, Generator):\n        response_gen = response\n        response = \"\"\n        for chunk in response_gen:\n            print(chunk, end=\"\", flush=True)\n            response += chunk\n    return response\n</code></pre>"},{"location":"code_docs/State/","title":"State","text":""},{"location":"code_docs/State/#src.utils.State.State","title":"<code>State</code>","text":"Source code in <code>src/utils/State.py</code> <pre><code>class State:\n    def __init__(\n        self,\n        idx,\n        reward_func: Callable = None,\n        reward_func_str: str = None,\n        policy=None,\n        perfomances: dict = None,\n    ):\n        \"\"\"\n        Represents a state in the reward function generation and evaluation process.\n\n        This class encapsulates the key components of a reward function's lifecycle,\n        tracking its index, implementation, policy, and performance metrics.\n\n        Attributes:\n            idx (int): Unique identifier for the state.\n            reward_func (Callable, optional): The compiled reward function.\n            reward_func_str (str, optional): String representation of the reward function.\n            policy (object, optional): The policy associated with the reward function.\n            performances (dict, optional): Performance metrics of the reward function.\n\n        Key Characteristics:\n            - Tracks the evolution of reward functions\n            - Provides a snapshot of a specific iteration\n            - Allows for dynamic updating of policy and performance\n\n        Initialization Constraints:\n            - Initial state (idx=0) cannot have a reward function\n            - Non-initial states must have both a reward function and its string representation\n\n        Methods:\n            - set_policy(policy): Update the associated policy\n            - set_performances(performances): Update performance metrics\n            - __repr__(): Provide a human-readable string representation of the state\n\n        Example:\n            # Creating a new state for a reward function\n            state = State(\n                idx=1, \n                reward_func=my_reward_func, \n                reward_func_str=\"def reward_func(...):\",\n                policy=None,\n                perfomances=None\n            )\n\n            # Updating state with training results\n            state.set_policy(trained_policy)\n            state.set_performances({\n                'success_rate': 0.75,\n                'average_reward': 10.5\n            })\n\n        Notes:\n            - Designed for tracking reward function iterations\n            - Provides flexibility in managing function states\n            - Supports logging and debugging of reward function generation process\n    \"\"\"\n\n        self.idx = idx\n        if self.idx == 0 and (reward_func is not None or reward_func_str is not None):\n            logger.error(\"the inital state don't take reward function\")\n        elif self.idx != 0 and (reward_func is None or reward_func_str is None):\n            logger.error(\"you need to give a reward function to the state\")\n        self.reward_func = reward_func\n        self.reward_func_str = reward_func_str\n        self.policy = policy\n        self.performances = perfomances\n\n    def set_policy(self, policy):\n        self.policy = policy\n\n    def set_performances(self, performances: dict):\n        self.performances = performances\n\n    def __repr__(self):\n        if self.performances is None:\n            repr = f\"state {self.idx}: \\nreward function: \\n\\n{self.reward_func_str}\\n\\n isn't trained yet\"\n        else:\n            repr = f\"state {self.idx}: \\nreward function: \\n\\n{self.reward_func_str}\\n\\n Performances: \\n\\n{self.performances}\\n\\n Policy: {self.policy}\"\n        return repr\n</code></pre>"},{"location":"code_docs/State/#src.utils.State.State.__init__","title":"<code>__init__(idx, reward_func=None, reward_func_str=None, policy=None, perfomances=None)</code>","text":"<p>Represents a state in the reward function generation and evaluation process.</p> <p>This class encapsulates the key components of a reward function's lifecycle, tracking its index, implementation, policy, and performance metrics.</p> <p>Attributes:</p> Name Type Description <code>idx</code> <code>int</code> <p>Unique identifier for the state.</p> <code>reward_func</code> <code>Callable</code> <p>The compiled reward function.</p> <code>reward_func_str</code> <code>str</code> <p>String representation of the reward function.</p> <code>policy</code> <code>object</code> <p>The policy associated with the reward function.</p> <code>performances</code> <code>dict</code> <p>Performance metrics of the reward function.</p> Key Characteristics <ul> <li>Tracks the evolution of reward functions</li> <li>Provides a snapshot of a specific iteration</li> <li>Allows for dynamic updating of policy and performance</li> </ul> Initialization Constraints <ul> <li>Initial state (idx=0) cannot have a reward function</li> <li>Non-initial states must have both a reward function and its string representation</li> </ul> <p>Functions:</p> Name Description <code>- set_policy</code> <p>Update the associated policy</p> <code>- set_performances</code> <p>Update performance metrics</p> <code>- __repr__</code> <p>Provide a human-readable string representation of the state</p> Example Notes <ul> <li>Designed for tracking reward function iterations</li> <li>Provides flexibility in managing function states</li> <li>Supports logging and debugging of reward function generation process</li> </ul> Source code in <code>src/utils/State.py</code> <pre><code>def __init__(\n    self,\n    idx,\n    reward_func: Callable = None,\n    reward_func_str: str = None,\n    policy=None,\n    perfomances: dict = None,\n):\n    \"\"\"\n    Represents a state in the reward function generation and evaluation process.\n\n    This class encapsulates the key components of a reward function's lifecycle,\n    tracking its index, implementation, policy, and performance metrics.\n\n    Attributes:\n        idx (int): Unique identifier for the state.\n        reward_func (Callable, optional): The compiled reward function.\n        reward_func_str (str, optional): String representation of the reward function.\n        policy (object, optional): The policy associated with the reward function.\n        performances (dict, optional): Performance metrics of the reward function.\n\n    Key Characteristics:\n        - Tracks the evolution of reward functions\n        - Provides a snapshot of a specific iteration\n        - Allows for dynamic updating of policy and performance\n\n    Initialization Constraints:\n        - Initial state (idx=0) cannot have a reward function\n        - Non-initial states must have both a reward function and its string representation\n\n    Methods:\n        - set_policy(policy): Update the associated policy\n        - set_performances(performances): Update performance metrics\n        - __repr__(): Provide a human-readable string representation of the state\n\n    Example:\n        # Creating a new state for a reward function\n        state = State(\n            idx=1, \n            reward_func=my_reward_func, \n            reward_func_str=\"def reward_func(...):\",\n            policy=None,\n            perfomances=None\n        )\n\n        # Updating state with training results\n        state.set_policy(trained_policy)\n        state.set_performances({\n            'success_rate': 0.75,\n            'average_reward': 10.5\n        })\n\n    Notes:\n        - Designed for tracking reward function iterations\n        - Provides flexibility in managing function states\n        - Supports logging and debugging of reward function generation process\n\"\"\"\n\n    self.idx = idx\n    if self.idx == 0 and (reward_func is not None or reward_func_str is not None):\n        logger.error(\"the inital state don't take reward function\")\n    elif self.idx != 0 and (reward_func is None or reward_func_str is None):\n        logger.error(\"you need to give a reward function to the state\")\n    self.reward_func = reward_func\n    self.reward_func_str = reward_func_str\n    self.policy = policy\n    self.performances = perfomances\n</code></pre>"},{"location":"code_docs/State/#src.utils.State.State.__init__--creating-a-new-state-for-a-reward-function","title":"Creating a new state for a reward function","text":"<p>state = State(     idx=1,      reward_func=my_reward_func,      reward_func_str=\"def reward_func(...):\",     policy=None,     perfomances=None )</p>"},{"location":"code_docs/State/#src.utils.State.State.__init__--updating-state-with-training-results","title":"Updating state with training results","text":"<p>state.set_policy(trained_policy) state.set_performances({     'success_rate': 0.75,     'average_reward': 10.5 })</p>"},{"location":"code_docs/VIRAL/","title":"Overview of VIRAL","text":""},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL","title":"<code>VIRAL</code>","text":"Source code in <code>src/VIRAL.py</code> <pre><code>class VIRAL:\n    def __init__(\n        self,\n        learning_algo: Algo,\n        env_type : Environments,\n        success_function: Callable,\n        objectives_metrics: List[callable] = [],\n        model: str = \"qwen2.5-coder\",\n        options: dict = {},\n    ):\n        \"\"\"\n        Initialize VIRAL architecture for dynamic reward function generation\n            Args:\n                model (str): Language model for reward generation\n                learning_method (str): Reinforcement learning method\n        \"\"\"\n        if options.get(\"seed\") is None:\n            options[\"seed\"] = random.randint(0, 1000000)\n\n        self.llm = OllamaChat(\n            model=model,\n            system_prompt=\"\"\"\n        You are an expert in Reinforcement Learning specialized in designing reward functions.\n        Strict criteria:\n        - Complete ONLY the reward function code\n        - Use Python format\n        - Give no additional explanations\n        - Focus on the Gymnasium environment \n        - Take into the observation of the state, the terminated and truncated boolean\n        - STOP immediately your completion after the last return\n        \"\"\",\n            options=options,\n        )\n        self.env_type : Environments = env_type\n        self.success_function = success_function\n        self.env = None\n        self.objectives_metrics = objectives_metrics\n        self.learning_algo: Algo = learning_algo\n        self.learning_method = None\n        self.logger = getLogger(\"VIRAL\")\n        # self._learning(self.memory[0])\n\n        if os.name == \"posix\":\n            self.queue = Queue()\n            self.memory: List[State] = [State(0)]\n            self.multi_process: list[Process] = []\n            self.multi_process.append(\n                Process(\n                    target=self._learning,\n                    args=(\n                        self.memory[0],\n                        self.queue,\n                    ),\n                )\n            )\n            self.multi_process[0].start()\n            self.to_get = 1\n        else:\n            self.memory: List[State] = [State(0)]\n\n\n    def generate_reward_function(\n        self, task_description: str, iterations: int = 1\n    ) -&gt; List[State]:\n        \"\"\"\n        Generate and iteratively improve a reward function using a Language Model (LLM).\n\n        This method implements a sophisticated reward function generation process \n        that involves multiple stages of creation, evaluation, and refinement.\n\n        Key Stages:\n            1. Initial Function Generation: Create two initial reward function candidates\n            2. Evaluation: Compare and identify the best and worst performing functions\n            3. Iterative Refinement: Progressively improve the worst-performing function\n\n        Args:\n            task_description (str): A detailed description of the task or environment \n                                    for which the reward function is being generated.\n            iterations (int, optional): Number of refinement iterations to perform. \n                                        Defaults to 1.\n\n        Returns:\n            List[State]: A list of generated and refined reward function states, \n                        containing information about each function's performance \n                        and implementation.\n\n        Process Overview:\n            - Generates two initial reward functions using an LLM\n            - Evaluates these functions using a policy evaluation method\n            - Selects the worst-performing function for refinement\n            - Iteratively refines the function through self-refinement\n            - Tracks the evolution of reward functions in the memory\n\n        Detailed Workflow:\n            1. Generate two initial reward functions\n                - Uses a predefined prompt template\n                - Applies configurable LLM generation options\n                - Compiles and tests each generated function\n            2. Evaluates initial functions\n                - Identifies best and worst performing functions\n            3. Iterative Refinement\n                - Applies self-refinement to the worst-performing function\n                - Re-evaluates after each refinement\n                - Repeats for specified number of iterations\n\n        Note:\n            - Uses dynamic LLM configuration options\n            - Supports flexible environment types\n            - Provides a systematic approach to reward function generation\n            - Logging at various stages for debugging and tracking\n        \"\"\"\n        # TODO Pourquoi additional_options ici et pas dans le constructeur ?\n        additional_options = {\n            \"temperature\": 1,\n            # \"num_predict\": 3, # l'impression que \u00e7a change rien a creuser\n            # \"mirostat\" : 1,\n            # \"mirostat_eta\" : 0.01, #g\u00e8re la vitesse de r\u00e9ponses du model (0.1 par d\u00e9faut) plus c'est petit plus c'est lent\n            # \"mirostat_tau\" : 4.0, #g\u00e8re la balance entre la diversit\u00e9 et la coherence des r\u00e9ponses (5.0 par d\u00e9faut) plus c'est petit plus c'est focus et coh\u00e9rent\n            # num_ctx\": 2048, # nombre de tokens contextuels (2048 par d\u00e9faut peut \u00eatre pas n\u00e9cessaire de changer)\n            # repeat_last_n\": 64, # combien le model regarde en arri\u00e8re pour \u00e9viter de r\u00e9p\u00e9ter les r\u00e9ponses (64 par d\u00e9faut large pour nous)\n            # \"repeat_penalty\": 1.5, # p\u00e9nalit\u00e9 pour \u00e9viter de r\u00e9p\u00e9ter les r\u00e9ponses (1.1 par d\u00e9faut au mac 1.5 int\u00e9ressant a modificer je pense)\n            # \"stop\": \"stop you here\" # pour stopper la g\u00e9n\u00e9ration de texte pas int\u00e9ressant pour nous\n            # \"tfs_z\": 1.2, #reduire l'impacte des token les moins \"pertinents\" (1.0 par d\u00e9faut pour d\u00e9sactiver 2.0 max)\n            # \"top_k\": 30, #reduit la probabilit\u00e9 de g\u00e9n\u00e9rer des non-sens (40 par d\u00e9faut, 100 pour g\u00e9n\u00e9rer des r\u00e9ponses plus diverses, 10 pour des r\u00e9ponses plus \"conservatrices\")\n            # \"top_p\": 0.95, #marche avec le top_k une forte valeur pour des texte plus diverses (0.9 par d\u00e9faut)\n            # \"min_p\": 0.05, #alternative au top_p, vise a s'a\u00e9ssurer de la balance entre qualit\u00e9 et diversit\u00e9 (0.0 par d\u00e9faut)\n            # \"seed\": 42, # a utiliser pour la reproductibilit\u00e9 des r\u00e9sultats (important si publication)\n        }\n        ### INIT STAGE ###\n        for i in [1, 2]:\n            prompt = f\"\"\"\n        Complete the reward function for a {self.env_type.value} environment.\n        Task Description: {task_description} Iteration {i+1}/{2}\n\n        complete this sentence:\n        def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -&gt; float:\n            \\\"\\\"\\\"Reward function for {self.env_type.value}\n\n            Args:\n                observations (np.ndarray): observation on the current state\n                terminated (bool): episode is terminated due a failure\n                truncated (bool): episode is truncated due a success\n\n            Returns:\n                float: The reward for the current step\n            \\\"\\\"\\\"\n        \"\"\"\n            self.llm.add_message(prompt)\n            response = self.llm.generate_response(\n                stream=True, additional_options=additional_options\n            )\n            self.logger.info(f\"additional options: {additional_options}\")\n            response = self.llm.print_Generator_and_return(response, i)\n            reward_func, response = self.get_runnable_function(response)\n            self.memory.append(State(i, reward_func, response))\n\n        best_idx, worst_idx = self.evaluate_policy(1, 2)\n        self.logger.debug(f\"state to refine: {worst_idx}\")\n        ### SECOND STAGE ###\n        for n in range(iterations - 1):\n            new_idx = self.self_refine_reward(worst_idx)\n            best_idx, worst_idx = self.evaluate_policy(best_idx, new_idx)\n            self.logger.debug(f\"state to refine: {worst_idx}\")\n        return self.memory\n\n    def get_code(self, response: str) -&gt; str:\n        \"\"\"\n        Clean and validate a code response by removing code block markers and ensuring a function definition.\n\n        This method is designed to process code responses, typically extracted from text or code blocks,\n        by performing the following operations:\\n\n        1. Remove leading and trailing code block markers (```),\n        2. Remove the 'python' language identifier,\n        3. Strip any additional whitespace\n        4. Validate that the response contains a function definition\n\n        Args:\n            response (str): The raw code response to be cleaned and validated.\n\n        Returns:\n            str: The cleaned code response containing a function definition.\n\n        Raises:\n            ValueError: If the response does not contain a valid function definition \n                        (i.e., if \"def \" is not present in the cleaned response).\n\n        Logging:\n            Logs the cleaned code at DEBUG level for debugging purposes.\n    \"\"\"\n        cleaned_response = response.strip(\"```\").replace(\"python\", \"\").strip()\n        if \"def \" not in cleaned_response:\n            raise ValueError(\"The answer does not contain a valid function definition.\")\n        self.logger.debug(\"Code nettoy\u00e9 pour compilation :\\n\" + cleaned_response)\n        return cleaned_response\n\n    def get_runnable_function(self, response: str, error: str = None) -&gt; Callable:\n        \"\"\"\n        Process and validate a reward function for a gym environment.\n\n        This method attempts to generate and validate a reward function by:\\n\n        1. Handling potential previous errors\n        2. Creating a gym environment\n        3. Cleaning and compiling the code\n        4. Testing the reward function with a sample action\n        5. Recursively handling various potential errors\n\n        Args:\n            response (str): The code response containing the reward function definition.\n            error (str, optional): Previous error message to be added to LLM context. \n                                    Defaults to None.\n\n        Returns:\n            tuple: A tuple containing:\n                - Callable: The compiled and validated reward function\n                - str: The original response code\n\n        Raises:\n            - ValueError: Invalid function definition\n            - SyntaxError: Syntax issues in the function\n            - RuntimeError: Execution problems during function testing\n\n        Note:\n            - Uses recursion to handle potential errors\n            - Relies on get_code, compile_reward_function, and test_reward_function methods\n            - Provides a robust mechanism for generating valid reward functions\n    \"\"\"\n        if error is not None:\n            self.llm.add_message(error)\n            response = self.llm.generate_response(stream=True)\n            response = self.llm.print_Generator_and_return(response)\n        try:\n            env = gym.make(self.env_type.value)\n            response = self.get_code(response)\n            reward_func = self.compile_reward_function(response)\n            state, _ = env.reset()\n            action = env.action_space.sample()\n            next_observation, _, terminated, truncated, _ = env.step(action)\n            self.test_reward_function(\n                reward_func,\n                observations=next_observation,\n                terminated=terminated,\n                truncated=truncated,\n            )\n        except ValueError as e:\n            self.logger.warning(str(e))\n            return self.get_runnable_function(response, str(e))\n        except SyntaxError as e:\n            self.logger.warning(f\"Error syntax {e}\")\n            return self.get_runnable_function(response, str(e))\n        except RuntimeError as e:\n            self.logger.warning(f\"Error execution {e}\")\n            return self.get_runnable_function(response, str(e))\n\n        return reward_func, response\n\n    def compile_reward_function(self, response: str) -&gt; Callable:\n        \"\"\"\n        Compile a reward function dynamically from a string response.\n\n        This method takes a code string representing a reward function and dynamically \n        compiles it into an executable Python function. It provides a secure way to \n        generate reward functions for reinforcement learning environments.\n\n        Key Features:\n            - Dynamically executes code in an isolated global namespace\n            - Provides access to NumPy functions\n            - Extracts the compiled function by its name\n            - Robust error handling for syntax issues\n\n        Args:\n            response (str): A string containing a complete Python function definition \n                            for a reward function.\n\n        Returns:\n            Callable: The compiled reward function that can be called with appropriate \n                    arguments in a gym environment.\n\n        Raises:\n            SyntaxError: If the provided code contains invalid Python syntax.\n            ValueError: If the function cannot be extracted from the compiled namespace.\n\n        Notes:\n            - Uses `exec()` for dynamic code compilation\n            - Provides NumPy (`np`) in the execution namespace\n            - Assumes the last function defined in the response is the reward function\n    \"\"\"\n\n        exec_globals = {}\n        exec_globals[\"np\"] = np\n        try:\n            exec(response, exec_globals)\n        except SyntaxError as e:\n            raise SyntaxError(f\"Syntax error in the generated code : {e}\")\n\n        reward_function_name = response.split(\"(\")[0].split()[\n            -1\n        ]  # r\u00e9cup le nom de la fonction\n        reward_function = exec_globals.get(reward_function_name)\n\n        return reward_function\n\n    def test_reward_function(self, reward_function: Callable, *args, **kwargs):\n        \"\"\"\n        Test the compiled reward function with provided inputs to validate its execution.\n\n        This method serves as a crucial validation step in the reward function generation \n        process. It attempts to execute the reward function with the given arguments and \n        logs the output or raises an error if execution fails.\n\n        Purpose:\n            - Verify the reward function can be executed without errors\n            - Log the reward function's output for debugging\n            - Ensure the function returns a valid result in the context of a gym environment\n\n        Args:\n            reward_function (Callable): The compiled reward function to be tested.\n            *args: Variable length argument list to pass to the reward function.\n                Typically includes observations, actions, or environment states.\n            **kwargs: Arbitrary keyword arguments to pass to the reward function.\n                May include additional context like 'terminated' or 'truncated' flags.\n\n        Raises:\n            RuntimeError: If the reward function fails to execute successfully.\n                This includes any exceptions that occur during function invocation.\n\n        Logging:\n            - Logs the reward function's output at DEBUG level when successful\n            - Provides detailed error information if execution fails\n\n        Notes:\n            - Designed to be flexible with varying function signatures\n            - Critical for validating dynamically generated reward functions\n            - Part of the reward function generation quality control process\n        \"\"\"\n        try:\n            reward = reward_function(*args, **kwargs)\n            self.logger.debug(f\"Reward function output: {reward}\")\n        except Exception as e:\n            raise RuntimeError(f\"Error during reward function execution: {e}\")\n\n    def self_refine_reward(self, idx: int) -&gt; Callable:\n        \"\"\"\n        Iteratively improve a reward function using self-refinement techniques.\n\n        This method implements an intelligent self-refinement process for reward functions\n        by leveraging a Language Model (LLM) to analyze and improve the current function\n        based on its previous performance.\n\n        Key Objectives: \n            - Analyze current reward function performance\n            - Generate an improved version of the reward function\n            - Maintain the core task objectives while optimizing the reward signal\n\n        Args:\n            idx (int): Index of the reward function in the memory to be refined.\n                    Typically the worst-performing function from previous evaluation.\n\n        Returns:\n            int: Index of the newly created refined reward function in the memory.\n\n        Refinement Process:\n            1. Construct a refinement prompt with:\n                - Current reward function code\n                - Performance metrics\n                - Explicit refinement goals\n            2. Generate a new reward function using LLM\n            3. Compile and validate the new function\n            4. Append the new function to memory\n            5. Return the index of the new function\n\n        Refinement Goals:\n            - Increase success rate of the policy\n            - Optimize the reward signal for better learning\n            - Preserve the original task objectives\n            - Improve overall performance\n\n        Notes:\n            - Uses the existing memory to track function evolution\n            - Leverages LLM for intelligent function refinement\n            - Provides a systematic approach to reward function improvement\n            - Maintains a history of function iterations\n    \"\"\"\n        refinement_prompt = f\"\"\"\n        improve the reward function to:\n        - Increase success rate\n        - Optimize reward signal\n        - Maintain task objectives\n\n        your best reward function:\n        {self.memory[idx].reward_func_str}\n\n        performance:\n        {self.memory[idx].performances}\n        \"\"\"\n\n        self.llm.add_message(refinement_prompt)\n        refined_response = self.llm.generate_response(stream=True)\n        refined_response = self.llm.print_Generator_and_return(refined_response)\n        reward_func, refined_response = self.get_runnable_function(refined_response)\n        self.memory.append(State(len(self.memory), reward_func, refined_response))\n\n        return len(self.memory) - 1\n\n    def _learning(self, state: State, queue: Queue = None) -&gt; None:\n        \"\"\"train a policy on an environment\"\"\"\n        self.logger.debug(f\"state {state.idx} begin is learning with reward function: {state.reward_func_str}\")\n        vec_env, model, numvenv = self._generate_env_model(state.reward_func)\n        training_callback = TrainingInfoCallback()\n        policy = model.learn(total_timesteps=60000, callback=training_callback)\n        policy.save(f\"model/policy{state.idx}.model\")\n        metrics = training_callback.get_metrics()\n        self.logger.debug(f\"{state.idx} TRAINING METRICS: {metrics}\")\n        sr_test = self.test_policy(vec_env, policy, numvenv)\n        # ajoute au dict metrics les performances sans ecraser les anciennes\n        metrics[\"test_success_rate\"] = sr_test\n        if os.name == \"posix\":\n            queue.put([state.idx, f\"model/policy{state.idx}.model\", metrics])\n        else:\n            state.set_performances(metrics)\n            self.memory[state.idx].set_policy(policy)\n\n    def evaluate_policy(self, idx1: int, idx2: int) -&gt; int:\n        \"\"\"\n        Evaluate policy performance for multiple reward functions\n\n        Args:\n            objectives_metrics (List[callable]): Custom objective metrics\n            num_episodes (int): Number of evaluation episodes\n\n        Returns:\n            Dict: Performance metrics for multiple reward functions\n        \"\"\"\n        if os.name == \"posix\":\n            if len(self.memory) &lt; 2:\n                self.logger.error(\"At least two reward functions are required.\")\n            to_join: list = []\n            for i in [idx1, idx2]:\n                if self.memory[i].performances is None:\n                    self.multi_process.append(\n                        Process(target=self._learning, args=(self.memory[i], self.queue))\n                    )\n                    self.multi_process[-1].start()\n                    self.to_get += 1\n                    to_join.append(len(self.multi_process)-1)\n\n            while self.to_get != 0:\n                try:\n                    get = self.queue.get(block=False)\n                    self.memory[get[0]].set_policy(get[1])\n                    self.memory[get[0]].set_performances(get[2])\n                    self.logger.debug(\n                        f\"state {get[0]} has finished learning with performances: {get[2]}\"\n                    )\n                    self.to_get -= 1\n                except Empty:\n                    sleep(0.1)\n\n            for p in to_join:\n                self.multi_process[p].join()\n            if (\n                self.memory[idx1].performances[\"test_success_rate\"]\n                &gt; self.memory[idx2].performances[\"test_success_rate\"]\n            ):\n                return idx1, idx2\n            else:\n                return idx2, idx1\n        else:\n            if len(self.memory) &lt; 2:\n                self.logger.error(\"At least two reward functions are required.\")\n            for i in [idx1, idx2]:\n                if self.memory[i].performances is None:\n                    self._learning(self.memory[i])\n            # TODO comparaison sur le success rate pour l'instant\n            if (\n                self.memory[idx1].performances[\"test_success_rate\"]\n                &gt; self.memory[idx2].performances[\"test_success_rate\"]\n            ):\n                return idx1, idx2\n            else:\n                return idx2, idx1\n\n    def test_policy(\n        self,\n        env,\n        policy,\n        numvenv,\n        nb_episodes=100,\n    ) -&gt; float:\n        all_rewards = []\n        nb_success = 0\n\n        obs = env.reset()\n\n        for _ in range(nb_episodes // numvenv):\n            episode_rewards = np.zeros(numvenv)\n            dones = [False] * numvenv\n\n            while not all(dones):\n                actions, _ = policy.predict(obs)\n                obs, rewards, new_dones, infos = env.step(actions)\n                episode_rewards += np.array(rewards)\n                for i, (done, info) in enumerate(zip(new_dones, infos)):\n                    if done:\n                        dones[i] = True\n                        if self.success_function(env.envs[i], info):\n                            nb_success += 1\n\n            all_rewards.extend(episode_rewards)\n\n        success_rate = nb_success / nb_episodes\n        return success_rate\n\n    def _generate_env_model(self, reward_func):\n        \"\"\"\n        Generate the environment model\n        \"\"\"\n        numenvs = 2\n        # SubprocVecEnv sauf on utilisera cuda derri\u00e8re\n        vec_env = make_vec_env(\n            self.env_type.value, \n            n_envs=numenvs, \n            wrapper_class=CustomRewardWrapper, \n            wrapper_kwargs={\"llm_reward_function\": reward_func})\n        if self.learning_algo == Algo.PPO:\n            model = PPO(\"MlpPolicy\", vec_env, verbose=1, device=\"cpu\")\n        else:\n            raise ValueError(\"The learning algorithm is not implemented.\")\n\n        return vec_env, model, numenvs\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.__init__","title":"<code>__init__(learning_algo, env_type, success_function, objectives_metrics=[], model='qwen2.5-coder', options={})</code>","text":"<p>Initialize VIRAL architecture for dynamic reward function generation     Args:         model (str): Language model for reward generation         learning_method (str): Reinforcement learning method</p> Source code in <code>src/VIRAL.py</code> <pre><code>def __init__(\n    self,\n    learning_algo: Algo,\n    env_type : Environments,\n    success_function: Callable,\n    objectives_metrics: List[callable] = [],\n    model: str = \"qwen2.5-coder\",\n    options: dict = {},\n):\n    \"\"\"\n    Initialize VIRAL architecture for dynamic reward function generation\n        Args:\n            model (str): Language model for reward generation\n            learning_method (str): Reinforcement learning method\n    \"\"\"\n    if options.get(\"seed\") is None:\n        options[\"seed\"] = random.randint(0, 1000000)\n\n    self.llm = OllamaChat(\n        model=model,\n        system_prompt=\"\"\"\n    You are an expert in Reinforcement Learning specialized in designing reward functions.\n    Strict criteria:\n    - Complete ONLY the reward function code\n    - Use Python format\n    - Give no additional explanations\n    - Focus on the Gymnasium environment \n    - Take into the observation of the state, the terminated and truncated boolean\n    - STOP immediately your completion after the last return\n    \"\"\",\n        options=options,\n    )\n    self.env_type : Environments = env_type\n    self.success_function = success_function\n    self.env = None\n    self.objectives_metrics = objectives_metrics\n    self.learning_algo: Algo = learning_algo\n    self.learning_method = None\n    self.logger = getLogger(\"VIRAL\")\n    # self._learning(self.memory[0])\n\n    if os.name == \"posix\":\n        self.queue = Queue()\n        self.memory: List[State] = [State(0)]\n        self.multi_process: list[Process] = []\n        self.multi_process.append(\n            Process(\n                target=self._learning,\n                args=(\n                    self.memory[0],\n                    self.queue,\n                ),\n            )\n        )\n        self.multi_process[0].start()\n        self.to_get = 1\n    else:\n        self.memory: List[State] = [State(0)]\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.compile_reward_function","title":"<code>compile_reward_function(response)</code>","text":"<p>Compile a reward function dynamically from a string response.</p> <p>This method takes a code string representing a reward function and dynamically  compiles it into an executable Python function. It provides a secure way to  generate reward functions for reinforcement learning environments.</p> Key Features <ul> <li>Dynamically executes code in an isolated global namespace</li> <li>Provides access to NumPy functions</li> <li>Extracts the compiled function by its name</li> <li>Robust error handling for syntax issues</li> </ul> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>A string containing a complete Python function definition              for a reward function.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The compiled reward function that can be called with appropriate      arguments in a gym environment.</p> <p>Raises:</p> Type Description <code>SyntaxError</code> <p>If the provided code contains invalid Python syntax.</p> <code>ValueError</code> <p>If the function cannot be extracted from the compiled namespace.</p> Notes <ul> <li>Uses <code>exec()</code> for dynamic code compilation</li> <li>Provides NumPy (<code>np</code>) in the execution namespace</li> <li>Assumes the last function defined in the response is the reward function</li> </ul> Source code in <code>src/VIRAL.py</code> <pre><code>def compile_reward_function(self, response: str) -&gt; Callable:\n    \"\"\"\n    Compile a reward function dynamically from a string response.\n\n    This method takes a code string representing a reward function and dynamically \n    compiles it into an executable Python function. It provides a secure way to \n    generate reward functions for reinforcement learning environments.\n\n    Key Features:\n        - Dynamically executes code in an isolated global namespace\n        - Provides access to NumPy functions\n        - Extracts the compiled function by its name\n        - Robust error handling for syntax issues\n\n    Args:\n        response (str): A string containing a complete Python function definition \n                        for a reward function.\n\n    Returns:\n        Callable: The compiled reward function that can be called with appropriate \n                arguments in a gym environment.\n\n    Raises:\n        SyntaxError: If the provided code contains invalid Python syntax.\n        ValueError: If the function cannot be extracted from the compiled namespace.\n\n    Notes:\n        - Uses `exec()` for dynamic code compilation\n        - Provides NumPy (`np`) in the execution namespace\n        - Assumes the last function defined in the response is the reward function\n\"\"\"\n\n    exec_globals = {}\n    exec_globals[\"np\"] = np\n    try:\n        exec(response, exec_globals)\n    except SyntaxError as e:\n        raise SyntaxError(f\"Syntax error in the generated code : {e}\")\n\n    reward_function_name = response.split(\"(\")[0].split()[\n        -1\n    ]  # r\u00e9cup le nom de la fonction\n    reward_function = exec_globals.get(reward_function_name)\n\n    return reward_function\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.evaluate_policy","title":"<code>evaluate_policy(idx1, idx2)</code>","text":"<p>Evaluate policy performance for multiple reward functions</p> <p>Parameters:</p> Name Type Description Default <code>objectives_metrics</code> <code>List[callable]</code> <p>Custom objective metrics</p> required <code>num_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>int</code> <p>Performance metrics for multiple reward functions</p> Source code in <code>src/VIRAL.py</code> <pre><code>def evaluate_policy(self, idx1: int, idx2: int) -&gt; int:\n    \"\"\"\n    Evaluate policy performance for multiple reward functions\n\n    Args:\n        objectives_metrics (List[callable]): Custom objective metrics\n        num_episodes (int): Number of evaluation episodes\n\n    Returns:\n        Dict: Performance metrics for multiple reward functions\n    \"\"\"\n    if os.name == \"posix\":\n        if len(self.memory) &lt; 2:\n            self.logger.error(\"At least two reward functions are required.\")\n        to_join: list = []\n        for i in [idx1, idx2]:\n            if self.memory[i].performances is None:\n                self.multi_process.append(\n                    Process(target=self._learning, args=(self.memory[i], self.queue))\n                )\n                self.multi_process[-1].start()\n                self.to_get += 1\n                to_join.append(len(self.multi_process)-1)\n\n        while self.to_get != 0:\n            try:\n                get = self.queue.get(block=False)\n                self.memory[get[0]].set_policy(get[1])\n                self.memory[get[0]].set_performances(get[2])\n                self.logger.debug(\n                    f\"state {get[0]} has finished learning with performances: {get[2]}\"\n                )\n                self.to_get -= 1\n            except Empty:\n                sleep(0.1)\n\n        for p in to_join:\n            self.multi_process[p].join()\n        if (\n            self.memory[idx1].performances[\"test_success_rate\"]\n            &gt; self.memory[idx2].performances[\"test_success_rate\"]\n        ):\n            return idx1, idx2\n        else:\n            return idx2, idx1\n    else:\n        if len(self.memory) &lt; 2:\n            self.logger.error(\"At least two reward functions are required.\")\n        for i in [idx1, idx2]:\n            if self.memory[i].performances is None:\n                self._learning(self.memory[i])\n        # TODO comparaison sur le success rate pour l'instant\n        if (\n            self.memory[idx1].performances[\"test_success_rate\"]\n            &gt; self.memory[idx2].performances[\"test_success_rate\"]\n        ):\n            return idx1, idx2\n        else:\n            return idx2, idx1\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.generate_reward_function","title":"<code>generate_reward_function(task_description, iterations=1)</code>","text":"<p>Generate and iteratively improve a reward function using a Language Model (LLM).</p> <p>This method implements a sophisticated reward function generation process  that involves multiple stages of creation, evaluation, and refinement.</p> Key Stages <ol> <li>Initial Function Generation: Create two initial reward function candidates</li> <li>Evaluation: Compare and identify the best and worst performing functions</li> <li>Iterative Refinement: Progressively improve the worst-performing function</li> </ol> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>A detailed description of the task or environment                      for which the reward function is being generated.</p> required <code>iterations</code> <code>int</code> <p>Number of refinement iterations to perform.                          Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>List[State]</code> <p>List[State]: A list of generated and refined reward function states,          containing information about each function's performance          and implementation.</p> Process Overview <ul> <li>Generates two initial reward functions using an LLM</li> <li>Evaluates these functions using a policy evaluation method</li> <li>Selects the worst-performing function for refinement</li> <li>Iteratively refines the function through self-refinement</li> <li>Tracks the evolution of reward functions in the memory</li> </ul> Detailed Workflow <ol> <li>Generate two initial reward functions<ul> <li>Uses a predefined prompt template</li> <li>Applies configurable LLM generation options</li> <li>Compiles and tests each generated function</li> </ul> </li> <li>Evaluates initial functions<ul> <li>Identifies best and worst performing functions</li> </ul> </li> <li>Iterative Refinement<ul> <li>Applies self-refinement to the worst-performing function</li> <li>Re-evaluates after each refinement</li> <li>Repeats for specified number of iterations</li> </ul> </li> </ol> Note <ul> <li>Uses dynamic LLM configuration options</li> <li>Supports flexible environment types</li> <li>Provides a systematic approach to reward function generation</li> <li>Logging at various stages for debugging and tracking</li> </ul> Source code in <code>src/VIRAL.py</code> <pre><code>def generate_reward_function(\n    self, task_description: str, iterations: int = 1\n) -&gt; List[State]:\n    \"\"\"\n    Generate and iteratively improve a reward function using a Language Model (LLM).\n\n    This method implements a sophisticated reward function generation process \n    that involves multiple stages of creation, evaluation, and refinement.\n\n    Key Stages:\n        1. Initial Function Generation: Create two initial reward function candidates\n        2. Evaluation: Compare and identify the best and worst performing functions\n        3. Iterative Refinement: Progressively improve the worst-performing function\n\n    Args:\n        task_description (str): A detailed description of the task or environment \n                                for which the reward function is being generated.\n        iterations (int, optional): Number of refinement iterations to perform. \n                                    Defaults to 1.\n\n    Returns:\n        List[State]: A list of generated and refined reward function states, \n                    containing information about each function's performance \n                    and implementation.\n\n    Process Overview:\n        - Generates two initial reward functions using an LLM\n        - Evaluates these functions using a policy evaluation method\n        - Selects the worst-performing function for refinement\n        - Iteratively refines the function through self-refinement\n        - Tracks the evolution of reward functions in the memory\n\n    Detailed Workflow:\n        1. Generate two initial reward functions\n            - Uses a predefined prompt template\n            - Applies configurable LLM generation options\n            - Compiles and tests each generated function\n        2. Evaluates initial functions\n            - Identifies best and worst performing functions\n        3. Iterative Refinement\n            - Applies self-refinement to the worst-performing function\n            - Re-evaluates after each refinement\n            - Repeats for specified number of iterations\n\n    Note:\n        - Uses dynamic LLM configuration options\n        - Supports flexible environment types\n        - Provides a systematic approach to reward function generation\n        - Logging at various stages for debugging and tracking\n    \"\"\"\n    # TODO Pourquoi additional_options ici et pas dans le constructeur ?\n    additional_options = {\n        \"temperature\": 1,\n        # \"num_predict\": 3, # l'impression que \u00e7a change rien a creuser\n        # \"mirostat\" : 1,\n        # \"mirostat_eta\" : 0.01, #g\u00e8re la vitesse de r\u00e9ponses du model (0.1 par d\u00e9faut) plus c'est petit plus c'est lent\n        # \"mirostat_tau\" : 4.0, #g\u00e8re la balance entre la diversit\u00e9 et la coherence des r\u00e9ponses (5.0 par d\u00e9faut) plus c'est petit plus c'est focus et coh\u00e9rent\n        # num_ctx\": 2048, # nombre de tokens contextuels (2048 par d\u00e9faut peut \u00eatre pas n\u00e9cessaire de changer)\n        # repeat_last_n\": 64, # combien le model regarde en arri\u00e8re pour \u00e9viter de r\u00e9p\u00e9ter les r\u00e9ponses (64 par d\u00e9faut large pour nous)\n        # \"repeat_penalty\": 1.5, # p\u00e9nalit\u00e9 pour \u00e9viter de r\u00e9p\u00e9ter les r\u00e9ponses (1.1 par d\u00e9faut au mac 1.5 int\u00e9ressant a modificer je pense)\n        # \"stop\": \"stop you here\" # pour stopper la g\u00e9n\u00e9ration de texte pas int\u00e9ressant pour nous\n        # \"tfs_z\": 1.2, #reduire l'impacte des token les moins \"pertinents\" (1.0 par d\u00e9faut pour d\u00e9sactiver 2.0 max)\n        # \"top_k\": 30, #reduit la probabilit\u00e9 de g\u00e9n\u00e9rer des non-sens (40 par d\u00e9faut, 100 pour g\u00e9n\u00e9rer des r\u00e9ponses plus diverses, 10 pour des r\u00e9ponses plus \"conservatrices\")\n        # \"top_p\": 0.95, #marche avec le top_k une forte valeur pour des texte plus diverses (0.9 par d\u00e9faut)\n        # \"min_p\": 0.05, #alternative au top_p, vise a s'a\u00e9ssurer de la balance entre qualit\u00e9 et diversit\u00e9 (0.0 par d\u00e9faut)\n        # \"seed\": 42, # a utiliser pour la reproductibilit\u00e9 des r\u00e9sultats (important si publication)\n    }\n    ### INIT STAGE ###\n    for i in [1, 2]:\n        prompt = f\"\"\"\n    Complete the reward function for a {self.env_type.value} environment.\n    Task Description: {task_description} Iteration {i+1}/{2}\n\n    complete this sentence:\n    def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -&gt; float:\n        \\\"\\\"\\\"Reward function for {self.env_type.value}\n\n        Args:\n            observations (np.ndarray): observation on the current state\n            terminated (bool): episode is terminated due a failure\n            truncated (bool): episode is truncated due a success\n\n        Returns:\n            float: The reward for the current step\n        \\\"\\\"\\\"\n    \"\"\"\n        self.llm.add_message(prompt)\n        response = self.llm.generate_response(\n            stream=True, additional_options=additional_options\n        )\n        self.logger.info(f\"additional options: {additional_options}\")\n        response = self.llm.print_Generator_and_return(response, i)\n        reward_func, response = self.get_runnable_function(response)\n        self.memory.append(State(i, reward_func, response))\n\n    best_idx, worst_idx = self.evaluate_policy(1, 2)\n    self.logger.debug(f\"state to refine: {worst_idx}\")\n    ### SECOND STAGE ###\n    for n in range(iterations - 1):\n        new_idx = self.self_refine_reward(worst_idx)\n        best_idx, worst_idx = self.evaluate_policy(best_idx, new_idx)\n        self.logger.debug(f\"state to refine: {worst_idx}\")\n    return self.memory\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.get_code","title":"<code>get_code(response)</code>","text":"<p>Clean and validate a code response by removing code block markers and ensuring a function definition.</p> <p>This method is designed to process code responses, typically extracted from text or code blocks, by performing the following operations:</p> <ol> <li>Remove leading and trailing code block markers (```),</li> <li>Remove the 'python' language identifier,</li> <li>Strip any additional whitespace</li> <li>Validate that the response contains a function definition</li> </ol> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The raw code response to be cleaned and validated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned code response containing a function definition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the response does not contain a valid function definition          (i.e., if \"def \" is not present in the cleaned response).</p> Logging <p>Logs the cleaned code at DEBUG level for debugging purposes.</p> Source code in <code>src/VIRAL.py</code> <pre><code>def get_code(self, response: str) -&gt; str:\n    \"\"\"\n    Clean and validate a code response by removing code block markers and ensuring a function definition.\n\n    This method is designed to process code responses, typically extracted from text or code blocks,\n    by performing the following operations:\\n\n    1. Remove leading and trailing code block markers (```),\n    2. Remove the 'python' language identifier,\n    3. Strip any additional whitespace\n    4. Validate that the response contains a function definition\n\n    Args:\n        response (str): The raw code response to be cleaned and validated.\n\n    Returns:\n        str: The cleaned code response containing a function definition.\n\n    Raises:\n        ValueError: If the response does not contain a valid function definition \n                    (i.e., if \"def \" is not present in the cleaned response).\n\n    Logging:\n        Logs the cleaned code at DEBUG level for debugging purposes.\n\"\"\"\n    cleaned_response = response.strip(\"```\").replace(\"python\", \"\").strip()\n    if \"def \" not in cleaned_response:\n        raise ValueError(\"The answer does not contain a valid function definition.\")\n    self.logger.debug(\"Code nettoy\u00e9 pour compilation :\\n\" + cleaned_response)\n    return cleaned_response\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.get_runnable_function","title":"<code>get_runnable_function(response, error=None)</code>","text":"<p>Process and validate a reward function for a gym environment.</p> <p>This method attempts to generate and validate a reward function by:</p> <ol> <li>Handling potential previous errors</li> <li>Creating a gym environment</li> <li>Cleaning and compiling the code</li> <li>Testing the reward function with a sample action</li> <li>Recursively handling various potential errors</li> </ol> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The code response containing the reward function definition.</p> required <code>error</code> <code>str</code> <p>Previous error message to be added to LLM context.                      Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Callable</code> <p>A tuple containing: - Callable: The compiled and validated reward function - str: The original response code</p> <p>Raises:</p> Type Description <code>-ValueError</code> <p>Invalid function definition</p> <code>-SyntaxError</code> <p>Syntax issues in the function</p> <code>-RuntimeError</code> <p>Execution problems during function testing</p> Note <ul> <li>Uses recursion to handle potential errors</li> <li>Relies on get_code, compile_reward_function, and test_reward_function methods</li> <li>Provides a robust mechanism for generating valid reward functions</li> </ul> Source code in <code>src/VIRAL.py</code> <pre><code>def get_runnable_function(self, response: str, error: str = None) -&gt; Callable:\n    \"\"\"\n    Process and validate a reward function for a gym environment.\n\n    This method attempts to generate and validate a reward function by:\\n\n    1. Handling potential previous errors\n    2. Creating a gym environment\n    3. Cleaning and compiling the code\n    4. Testing the reward function with a sample action\n    5. Recursively handling various potential errors\n\n    Args:\n        response (str): The code response containing the reward function definition.\n        error (str, optional): Previous error message to be added to LLM context. \n                                Defaults to None.\n\n    Returns:\n        tuple: A tuple containing:\n            - Callable: The compiled and validated reward function\n            - str: The original response code\n\n    Raises:\n        - ValueError: Invalid function definition\n        - SyntaxError: Syntax issues in the function\n        - RuntimeError: Execution problems during function testing\n\n    Note:\n        - Uses recursion to handle potential errors\n        - Relies on get_code, compile_reward_function, and test_reward_function methods\n        - Provides a robust mechanism for generating valid reward functions\n\"\"\"\n    if error is not None:\n        self.llm.add_message(error)\n        response = self.llm.generate_response(stream=True)\n        response = self.llm.print_Generator_and_return(response)\n    try:\n        env = gym.make(self.env_type.value)\n        response = self.get_code(response)\n        reward_func = self.compile_reward_function(response)\n        state, _ = env.reset()\n        action = env.action_space.sample()\n        next_observation, _, terminated, truncated, _ = env.step(action)\n        self.test_reward_function(\n            reward_func,\n            observations=next_observation,\n            terminated=terminated,\n            truncated=truncated,\n        )\n    except ValueError as e:\n        self.logger.warning(str(e))\n        return self.get_runnable_function(response, str(e))\n    except SyntaxError as e:\n        self.logger.warning(f\"Error syntax {e}\")\n        return self.get_runnable_function(response, str(e))\n    except RuntimeError as e:\n        self.logger.warning(f\"Error execution {e}\")\n        return self.get_runnable_function(response, str(e))\n\n    return reward_func, response\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.self_refine_reward","title":"<code>self_refine_reward(idx)</code>","text":"<p>Iteratively improve a reward function using self-refinement techniques.</p> <p>This method implements an intelligent self-refinement process for reward functions by leveraging a Language Model (LLM) to analyze and improve the current function based on its previous performance.</p> Key Objectives <ul> <li>Analyze current reward function performance</li> <li>Generate an improved version of the reward function</li> <li>Maintain the core task objectives while optimizing the reward signal</li> </ul> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the reward function in the memory to be refined.     Typically the worst-performing function from previous evaluation.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>Callable</code> <p>Index of the newly created refined reward function in the memory.</p> Refinement Process <ol> <li>Construct a refinement prompt with:<ul> <li>Current reward function code</li> <li>Performance metrics</li> <li>Explicit refinement goals</li> </ul> </li> <li>Generate a new reward function using LLM</li> <li>Compile and validate the new function</li> <li>Append the new function to memory</li> <li>Return the index of the new function</li> </ol> Refinement Goals <ul> <li>Increase success rate of the policy</li> <li>Optimize the reward signal for better learning</li> <li>Preserve the original task objectives</li> <li>Improve overall performance</li> </ul> Notes <ul> <li>Uses the existing memory to track function evolution</li> <li>Leverages LLM for intelligent function refinement</li> <li>Provides a systematic approach to reward function improvement</li> <li>Maintains a history of function iterations</li> </ul> Source code in <code>src/VIRAL.py</code> <pre><code>def self_refine_reward(self, idx: int) -&gt; Callable:\n    \"\"\"\n    Iteratively improve a reward function using self-refinement techniques.\n\n    This method implements an intelligent self-refinement process for reward functions\n    by leveraging a Language Model (LLM) to analyze and improve the current function\n    based on its previous performance.\n\n    Key Objectives: \n        - Analyze current reward function performance\n        - Generate an improved version of the reward function\n        - Maintain the core task objectives while optimizing the reward signal\n\n    Args:\n        idx (int): Index of the reward function in the memory to be refined.\n                Typically the worst-performing function from previous evaluation.\n\n    Returns:\n        int: Index of the newly created refined reward function in the memory.\n\n    Refinement Process:\n        1. Construct a refinement prompt with:\n            - Current reward function code\n            - Performance metrics\n            - Explicit refinement goals\n        2. Generate a new reward function using LLM\n        3. Compile and validate the new function\n        4. Append the new function to memory\n        5. Return the index of the new function\n\n    Refinement Goals:\n        - Increase success rate of the policy\n        - Optimize the reward signal for better learning\n        - Preserve the original task objectives\n        - Improve overall performance\n\n    Notes:\n        - Uses the existing memory to track function evolution\n        - Leverages LLM for intelligent function refinement\n        - Provides a systematic approach to reward function improvement\n        - Maintains a history of function iterations\n\"\"\"\n    refinement_prompt = f\"\"\"\n    improve the reward function to:\n    - Increase success rate\n    - Optimize reward signal\n    - Maintain task objectives\n\n    your best reward function:\n    {self.memory[idx].reward_func_str}\n\n    performance:\n    {self.memory[idx].performances}\n    \"\"\"\n\n    self.llm.add_message(refinement_prompt)\n    refined_response = self.llm.generate_response(stream=True)\n    refined_response = self.llm.print_Generator_and_return(refined_response)\n    reward_func, refined_response = self.get_runnable_function(refined_response)\n    self.memory.append(State(len(self.memory), reward_func, refined_response))\n\n    return len(self.memory) - 1\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.test_reward_function","title":"<code>test_reward_function(reward_function, *args, **kwargs)</code>","text":"<p>Test the compiled reward function with provided inputs to validate its execution.</p> <p>This method serves as a crucial validation step in the reward function generation  process. It attempts to execute the reward function with the given arguments and  logs the output or raises an error if execution fails.</p> Purpose <ul> <li>Verify the reward function can be executed without errors</li> <li>Log the reward function's output for debugging</li> <li>Ensure the function returns a valid result in the context of a gym environment</li> </ul> <p>Parameters:</p> Name Type Description Default <code>reward_function</code> <code>Callable</code> <p>The compiled reward function to be tested.</p> required <code>*args</code> <p>Variable length argument list to pass to the reward function. Typically includes observations, actions, or environment states.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments to pass to the reward function. May include additional context like 'terminated' or 'truncated' flags.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the reward function fails to execute successfully. This includes any exceptions that occur during function invocation.</p> Logging <ul> <li>Logs the reward function's output at DEBUG level when successful</li> <li>Provides detailed error information if execution fails</li> </ul> Notes <ul> <li>Designed to be flexible with varying function signatures</li> <li>Critical for validating dynamically generated reward functions</li> <li>Part of the reward function generation quality control process</li> </ul> Source code in <code>src/VIRAL.py</code> <pre><code>def test_reward_function(self, reward_function: Callable, *args, **kwargs):\n    \"\"\"\n    Test the compiled reward function with provided inputs to validate its execution.\n\n    This method serves as a crucial validation step in the reward function generation \n    process. It attempts to execute the reward function with the given arguments and \n    logs the output or raises an error if execution fails.\n\n    Purpose:\n        - Verify the reward function can be executed without errors\n        - Log the reward function's output for debugging\n        - Ensure the function returns a valid result in the context of a gym environment\n\n    Args:\n        reward_function (Callable): The compiled reward function to be tested.\n        *args: Variable length argument list to pass to the reward function.\n            Typically includes observations, actions, or environment states.\n        **kwargs: Arbitrary keyword arguments to pass to the reward function.\n            May include additional context like 'terminated' or 'truncated' flags.\n\n    Raises:\n        RuntimeError: If the reward function fails to execute successfully.\n            This includes any exceptions that occur during function invocation.\n\n    Logging:\n        - Logs the reward function's output at DEBUG level when successful\n        - Provides detailed error information if execution fails\n\n    Notes:\n        - Designed to be flexible with varying function signatures\n        - Critical for validating dynamically generated reward functions\n        - Part of the reward function generation quality control process\n    \"\"\"\n    try:\n        reward = reward_function(*args, **kwargs)\n        self.logger.debug(f\"Reward function output: {reward}\")\n    except Exception as e:\n        raise RuntimeError(f\"Error during reward function execution: {e}\")\n</code></pre>"},{"location":"code_docs/main/","title":"Main","text":""},{"location":"code_docs/main/#src.main.main","title":"<code>main()</code>","text":"<p>Main entry point of the script.</p> <p>This block is executed when the script is run directly. It initializes the logger, and run VIRAL. It uses CLI interface. memory.</p> Source code in <code>src/main.py</code> <pre><code>def main():\n    \"\"\"\n    Main entry point of the script.\n\n    This block is executed when the script is run directly. It initializes the\n    logger, and run VIRAL. It uses CLI interface.\n    memory.\n    \"\"\"\n    logger = parse_logger()\n    viral = VIRAL(\n        learning_algo=Algo.PPO,\n        env_type=Environments.CARTPOLE, \n        success_function=Environments.CARTPOLE.task_function)\n    res = viral.generate_reward_function(\n        task_description=Environments.CARTPOLE.task_description,\n        iterations=2,\n    )\n    for state in viral.memory:\n        logger.info(state)\n</code></pre>"},{"location":"code_docs/main/#src.main.parse_logger","title":"<code>parse_logger()</code>","text":"<p>Parses command-line arguments to configure the logger.</p> <p>Returns:</p> Name Type Description <code>Logger</code> <p>Configured logger instance.</p> Source code in <code>src/main.py</code> <pre><code>def parse_logger():\n    \"\"\"\n    Parses command-line arguments to configure the logger.\n\n    Returns:\n        Logger: Configured logger instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose mode\"\n    )\n    args = parser.parse_args()\n\n    if args.verbose:\n        init_logger(\"DEBUG\")\n        print(\"Verbose mode enabled\")\n    else:\n        init_logger(\"DEBUG\")\n\n    return getLogger()\n</code></pre>"},{"location":"code_docs/utils/","title":"Utils","text":""},{"location":"code_docs/utils/#src.utils.utils.plot_sumrwdperepi","title":"<code>plot_sumrwdperepi(sum_rewards)</code>","text":"<p>trace courbe de somme des rec par episodes</p> Source code in <code>src/utils/utils.py</code> <pre><code>def plot_sumrwdperepi(sum_rewards: list):\n    \"trace courbe de somme des rec par episodes\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.plot(np.arange(len(sum_rewards)), sum_rewards)\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Episode #\")\n    plt.show()\n</code></pre>"},{"location":"code_docs/utils/#src.utils.utils.plot_sumrwdperepi_movingavg","title":"<code>plot_sumrwdperepi_movingavg(sum_rewards, avgs)</code>","text":"<p>trace courbe de somme des rec (sum_rewards) et moyenne glissante (avgs) par episodes</p> Source code in <code>src/utils/utils.py</code> <pre><code>def plot_sumrwdperepi_movingavg(sum_rewards: list, avgs: list):\n    \"trace courbe de somme des rec (sum_rewards) et moyenne glissante (avgs) par episodes\"\n    print(\"sum_rwd:\", type(sum_rewards))\n    print(\"avgs:\", type(avgs))\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.plot(np.arange(len(sum_rewards)), sum_rewards, label=\"sum_rwd\")\n    plt.plot(np.arange(len(avgs)), avgs, c=\"r\", label=\"average\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Episode #\")\n    plt.legend(loc=\"upper left\")\n    plt.show()\n</code></pre>"},{"location":"code_docs/utils/#src.utils.utils.plot_sumrwdperepi_overseed2","title":"<code>plot_sumrwdperepi_overseed2(rewards_over_seeds)</code>","text":"<p>trace courbe de somme des rec par episodes moyenne + std sur plusieurs seeds</p> Source code in <code>src/utils/utils.py</code> <pre><code>def plot_sumrwdperepi_overseed2(rewards_over_seeds: list):\n    \"\"\"\n    trace courbe de somme des rec par episodes moyenne + std sur plusieurs seeds\n\n    \"\"\"\n    # rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n    df1 = pd.DataFrame(rewards_over_seeds).melt()\n    df1.rename(columns={\"variable\": \"episodes\", \"value\": \"rwd\"}, inplace=True)\n    sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n    sns.lineplot(x=\"episodes\", y=\"rwd\", data=df1, estimator=np.mean, errorbar=\"sd\").set(\n        title=\"\"\n    )\n    plt.show()\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/","title":"DirectSearch","text":""},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch","title":"<code>DirectSearch</code>","text":"Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>class DirectSearch:\n    def __init__(\n        self, env, nb_episodes: int = 2000, max_t: int = 1000, det: int = True\n    ):\n        \"\"\"\n        env: Environnement Gymnasium\n        det: Si la politique est d\u00e9terministe ou stochastique\n\n        \"\"\"\n        self.dim_entree = env.observation_space.shape[0]\n        self.dim_sortie = env.action_space.n\n        self.det = det\n        self.nb_episodes = nb_episodes\n        self.max_t = max_t\n        self.env = env\n        # Matrice entre * sortie\n        self.poids = np.random.rand(self.dim_entree, self.dim_sortie)\n\n    def __repr__(self):\n        return \"DirectSearch\"\n\n    def output(self, etat: np.ndarray) -&gt; int:\n        \"\"\"Calcul de la sortie de la politique\n        - si d\u00e9terministe : argmax\n        - si stochastique : probabilit\u00e9 de chaque action\n        \"\"\"\n        # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n        prob = torch.nn.functional.softmax(torch.tensor(etat.dot(self.poids)), dim=0)\n        if self.det:\n            return torch.argmax(prob).item()\n        else:\n            return torch.Categorical(probs=prob).sample().item()\n\n    def set_poids(self, poids: np.ndarray):\n        self.poids = poids\n\n    def get_poids(self) -&gt; np.ndarray:\n        return self.poids\n\n    def save(self, file):\n        f = open(file, \"w\")\n        f.write(f\"{self.poids};{self.det}\")\n\n    def load(self, file):\n        f = open(file, \"r\")\n        param = f.read().split(\";\")\n        self.poids = param[0]\n        self.det = param[1]\n\n    def rollout(self, reward_func) -&gt; int:\n        \"\"\"\n        execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode\n        \"\"\"\n        total_rec = 0\n        is_success = False\n        state, _ = self.env.reset()\n        for _ in range(1, self.max_t + 1):\n            action = self.output(state)\n            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n            if reward_func is not None:\n                reward = reward_func(next_observation, terminated, truncated)\n            total_rec += reward\n            state = next_observation\n            if terminated:\n                return total_rec, is_success\n            if truncated:\n                is_success = True\n                return total_rec, is_success\n        return total_rec, is_success\n\n    def train(\n        self, reward_func=None, save_name=\"\", stop: threading.Event | None = None\n    ) -&gt; tuple[list, np.ndarray]:\n        cp_policy: DirectSearch = deepcopy(self)\n        bruit_std = 1e-2\n        meilleur_perf = 0\n        meilleur_poid = cp_policy.get_poids()\n        perf_by_episode = list()\n        nb_best_perf = 0\n        nb_success = 0\n        for i_episode in range(1, self.nb_episodes + 1):\n            if stop is not None:\n                if stop.is_set():\n                    break\n            perf, success = cp_policy.rollout(reward_func)\n            nb_success += success\n            perf_by_episode.append(perf)\n\n            if perf == meilleur_perf:\n                nb_best_perf += 1\n            else:\n                nb_best_perf = 0\n            if nb_best_perf == 10:\n                break\n            if perf &gt;= meilleur_perf:\n                meilleur_perf = perf\n                meilleur_poid = cp_policy.get_poids()\n                if perf &gt; meilleur_perf:\n                    nb_best_perf = 0\n                # reduction de la variance du bruit\n                bruit_std = max(1e-3, bruit_std / 2)\n            else:\n                # augmentation de la variance du bruit\n                bruit_std = min(2, bruit_std * 2)\n            # On calcule le bruit en fonction de la variance\n            bruit = np.random.normal(\n                0, bruit_std, cp_policy.dim_entree * cp_policy.dim_sortie\n            )\n            # Reshape le bruit pour qu'il ait la m\u00eame taille que les poids\n            bruit = bruit.reshape(cp_policy.dim_entree, cp_policy.dim_sortie)\n            # On ajoute le bruit aux poids\n            cp_policy.set_poids(meilleur_poid + bruit)\n        if save_name is not None:\n            cp_policy.save(save_name)\n        return cp_policy, perf_by_episode, (nb_success / i_episode), i_episode\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.__init__","title":"<code>__init__(env, nb_episodes=2000, max_t=1000, det=True)</code>","text":"<p>env: Environnement Gymnasium det: Si la politique est d\u00e9terministe ou stochastique</p> Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>def __init__(\n    self, env, nb_episodes: int = 2000, max_t: int = 1000, det: int = True\n):\n    \"\"\"\n    env: Environnement Gymnasium\n    det: Si la politique est d\u00e9terministe ou stochastique\n\n    \"\"\"\n    self.dim_entree = env.observation_space.shape[0]\n    self.dim_sortie = env.action_space.n\n    self.det = det\n    self.nb_episodes = nb_episodes\n    self.max_t = max_t\n    self.env = env\n    # Matrice entre * sortie\n    self.poids = np.random.rand(self.dim_entree, self.dim_sortie)\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.output","title":"<code>output(etat)</code>","text":"<p>Calcul de la sortie de la politique - si d\u00e9terministe : argmax - si stochastique : probabilit\u00e9 de chaque action</p> Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>def output(self, etat: np.ndarray) -&gt; int:\n    \"\"\"Calcul de la sortie de la politique\n    - si d\u00e9terministe : argmax\n    - si stochastique : probabilit\u00e9 de chaque action\n    \"\"\"\n    # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n    prob = torch.nn.functional.softmax(torch.tensor(etat.dot(self.poids)), dim=0)\n    if self.det:\n        return torch.argmax(prob).item()\n    else:\n        return torch.Categorical(probs=prob).sample().item()\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.rollout","title":"<code>rollout(reward_func)</code>","text":"<p>execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode</p> Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>def rollout(self, reward_func) -&gt; int:\n    \"\"\"\n    execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode\n    \"\"\"\n    total_rec = 0\n    is_success = False\n    state, _ = self.env.reset()\n    for _ in range(1, self.max_t + 1):\n        action = self.output(state)\n        next_observation, reward, terminated, truncated, _ = self.env.step(action)\n        if reward_func is not None:\n            reward = reward_func(next_observation, terminated, truncated)\n        total_rec += reward\n        state = next_observation\n        if terminated:\n            return total_rec, is_success\n        if truncated:\n            is_success = True\n            return total_rec, is_success\n    return total_rec, is_success\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/","title":"PPO","text":""},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO","title":"<code>PPO</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>class PPO(nn.Module):\n    def __init__(\n        self,\n        env: gym.Env,\n        hidden_size: int = 256,\n        std: float = 0.0,\n        batch_size: int = 1,\n        ppo_epoch: int = 4,\n        lr: float = 3e-4,\n        nb_episodes: int = 2000,\n        max_t: int = 1000,\n    ):\n        \"\"\" \"\"\"\n        super(PPO, self).__init__()\n        self.num_inputs = env.observation_space.shape[0]\n        self.num_outputs = env.action_space.n\n        self.critic = nn.Sequential(\n            nn.Linear(self.num_inputs, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1),\n        )  # predict the reward from a state\n\n        self.actor = nn.Sequential(\n            nn.Linear(self.num_inputs, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, self.num_outputs),\n        )  # predict the action todo\n        self.log_std = nn.Parameter(\n            torch.ones(1, self.num_outputs) * std\n        )  # compute the std\n\n        def init_weights(m):\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.1)\n                nn.init.constant_(m.bias, 0.1)\n\n        self.apply(init_weights)  # help to converge\n\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.nb_episodes = nb_episodes\n        self.max_t = max_t\n        self.ppo_epochs = ppo_epoch\n        self.lr = lr\n        self.batch_size = batch_size\n        self.env = env\n\n    def forward(self, x) -&gt; tuple[Normal, float]:\n        if isinstance(x, np.ndarray):\n            x = torch.Tensor(x)\n        value = self.critic(x)  # predict the reward\n        mu = self.actor(x)  # predict the action\n        # std = self.log_std.exp()  # compute std\n        dist = Categorical(logits=mu.detach())  # make a normal distribution of proba\n        return dist, value\n\n    def __repr__(self) -&gt; str:\n        return \"PPO\"\n\n    def output(self, state: np.ndarray) -&gt; int:\n        \"\"\" \"\"\"\n        dist, _ = self.forward(state)\n        action = dist.sample()\n        action = torch.argmax(action)\n        return action.item()\n\n    def save(self, file: str):\n        \"\"\"\n        save the model.\n        \"\"\"\n        torch.save(self.state_dict(), file)\n\n    def load(self, file: str):\n        \"\"\"\n        load the model form file\n        \"\"\"\n        self.load_state_dict(torch.load(file))\n\n    def train(\n        self,\n        reward_func=None,\n        save_name: str = \"\",\n        stop: threading.Event | None = None,\n    ) -&gt; tuple[dict, list, float, int]:\n        # Sauvegarde des param\u00e8tres initiaux\n        cp_policy = deepcopy(self)\n\n        # Initialisation des m\u00e9triques\n        recompenses = []\n        a_la_suite = 0\n        nb_success = 0\n        score_max = 0\n\n        state, _ = cp_policy.env.reset()\n        for i_episode in range(cp_policy.nb_episodes):\n            if stop is not None:\n                if stop.is_set():\n                    break\n            log_probs = []\n            values = []\n            states = []\n            actions = []\n            rewards = []\n            # Run policy T times\n            for t in range(cp_policy.max_t):\n                dist, value = cp_policy.forward(state)\n                action = dist.sample()\n                log_prob = dist.log_prob(action)\n                next_state, reward, terminated, truncated, _ = cp_policy.env.step(action.item())\n                log_probs.append(torch.tensor([log_prob]))\n                values.append(value)\n                actions.append(torch.tensor([action]))\n                rewards.append(reward)\n                states.append(torch.Tensor(state))\n                # actions.append(action)\n                state = next_state\n                if terminated:\n                    break\n                if truncated:\n                    nb_success +=1\n                    break\n            # compute \u00c2_1 ... \u00c2_t\n            _, value = cp_policy.forward(state)\n            values.append(value)\n            returns = cp_policy._global_advantage_estimates(\n                rewards, values\n            )\n            returns = torch.cat(returns).detach()\n            log_probs = torch.cat(log_probs).detach()\n            values    = torch.cat(values).detach()\n            states    = torch.cat(states)\n            actions   = torch.cat(actions)\n            advantage_estimates = returns - values[-1:]\n            # optimise\n            dataset = cp_policy.PPODataset(\n                states, actions, log_probs, returns, advantage_estimates\n            )\n            print(f\"Dataset size: {len(dataset)}\")\n            print(f\"DataLoader batch size: {cp_policy.batch_size}\")\n            data_loader = DataLoader(dataset, batch_size=cp_policy.batch_size, shuffle=True)\n            cp_policy._ppo_update(data_loader)\n        if save_name is not None:\n            cp_policy.save(save_name)\n        return cp_policy, rewards, (nb_success / i_episode), i_episode\n\n    def _global_advantage_estimates(self, rewards, values, gamma=0.99, tau=0.95):\n        gae = 0\n        returns = []\n        for step in reversed(range(len(rewards))):\n            delta = rewards[step] + gamma * values[step + 1] - values[step]\n            gae = delta + gamma * tau * gae\n            returns.insert(0, gae + values[step])\n        return returns\n\n    def _ppo_update(\n        self, dataloader: DataLoader):\n        for _ in range(self.ppo_epochs):\n            for state, action, old_log_probs, return_, advantage in dataloader:\n                dist, value = self.forward(state)\n                entropy = dist.entropy().mean()\n                new_log_probs = dist.log_prob(action)\n\n                ratio = (new_log_probs - old_log_probs).exp()\n                surr1 = ratio * advantage\n                surr2 = (\n                    torch.clamp(ratio, 0.8, 1.2) * advantage\n                )\n\n                actor_loss = -torch.min(surr1, surr2).mean()\n                critic_loss = (return_ - value).pow(2).mean()\n\n                loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n    class PPODataset(Dataset):\n        def __init__(self, states, actions, log_probs, returns, advantages):\n            self.states = states\n            self.actions = actions\n            self.log_probs = log_probs\n            self.returns = returns\n            self.advantages = advantages\n\n        def __len__(self):\n            return len(self.actions)\n\n        def __getitem__(self, idx):\n            print(f\"Accessing index {idx}\")\n            return (\n                self.states[idx],\n                self.actions[idx],\n                self.log_probs[idx],\n                self.returns[idx],\n                self.advantages[idx],\n            )\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.__init__","title":"<code>__init__(env, hidden_size=256, std=0.0, batch_size=1, ppo_epoch=4, lr=0.0003, nb_episodes=2000, max_t=1000)</code>","text":"Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def __init__(\n    self,\n    env: gym.Env,\n    hidden_size: int = 256,\n    std: float = 0.0,\n    batch_size: int = 1,\n    ppo_epoch: int = 4,\n    lr: float = 3e-4,\n    nb_episodes: int = 2000,\n    max_t: int = 1000,\n):\n    \"\"\" \"\"\"\n    super(PPO, self).__init__()\n    self.num_inputs = env.observation_space.shape[0]\n    self.num_outputs = env.action_space.n\n    self.critic = nn.Sequential(\n        nn.Linear(self.num_inputs, hidden_size),\n        nn.ReLU(),\n        nn.Linear(hidden_size, 1),\n    )  # predict the reward from a state\n\n    self.actor = nn.Sequential(\n        nn.Linear(self.num_inputs, hidden_size),\n        nn.ReLU(),\n        nn.Linear(hidden_size, self.num_outputs),\n    )  # predict the action todo\n    self.log_std = nn.Parameter(\n        torch.ones(1, self.num_outputs) * std\n    )  # compute the std\n\n    def init_weights(m):\n        if isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, mean=0.0, std=0.1)\n            nn.init.constant_(m.bias, 0.1)\n\n    self.apply(init_weights)  # help to converge\n\n    self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n    self.nb_episodes = nb_episodes\n    self.max_t = max_t\n    self.ppo_epochs = ppo_epoch\n    self.lr = lr\n    self.batch_size = batch_size\n    self.env = env\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.load","title":"<code>load(file)</code>","text":"<p>load the model form file</p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def load(self, file: str):\n    \"\"\"\n    load the model form file\n    \"\"\"\n    self.load_state_dict(torch.load(file))\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.output","title":"<code>output(state)</code>","text":"Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def output(self, state: np.ndarray) -&gt; int:\n    \"\"\" \"\"\"\n    dist, _ = self.forward(state)\n    action = dist.sample()\n    action = torch.argmax(action)\n    return action.item()\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.save","title":"<code>save(file)</code>","text":"<p>save the model.</p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def save(self, file: str):\n    \"\"\"\n    save the model.\n    \"\"\"\n    torch.save(self.state_dict(), file)\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.learning","title":"<code>learning(learning_method, env)</code>","text":"<p>train a policy on an environment</p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def learning(learning_method, env) -&gt; None:\n    \"\"\"train a policy on an environment\"\"\"\n    policy, _, sr, nb_ep = learning_method.train()\n    _, rewards, sr_test = test_policy(policy, env)\n    print(\n        {\n            \"train_success_rate\": sr,\n            \"train_episodes\": nb_ep,\n            \"test_success_rate\": sr_test,\n            \"test_rewards\": rewards,\n        }\n    )\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/","title":"Reinforce","text":""},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce","title":"<code>Reinforce</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>class Reinforce(nn.Module):\n    def __init__(\n        self,\n        env,\n        couche_cachee: list,\n        nb_episodes: int = 2000,\n        max_t: int = 1000,\n        gamma: float = 0.99,\n    ):\n        \"\"\"\n        env: Environnement Gymnasium\n        couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32])\n        gamma: Facteur de discount pour le calcul des retours cumul\u00e9s\n        det: Si la politique est d\u00e9terministe ou stochastique\n        \"\"\"\n        super(Reinforce, self).__init__()\n        self.dim_entree = env.observation_space.shape[0]\n        self.dim_sortie = env.action_space.n\n        self.gamma = gamma\n        self.env = env\n\n        self.nb_episodes = nb_episodes\n        self.max_t = max_t\n\n        # Cr\u00e9er dynamiquement les couches cach\u00e9es\n        self.couches_cachees = []\n        input_size = self.dim_entree\n        for hidden_size in couche_cachee:\n            self.couches_cachees.append(nn.Linear(input_size, hidden_size))\n            input_size = hidden_size\n        self.couches_cachees = nn.ModuleList(self.couches_cachees)\n\n        # La derni\u00e8re couche qui produit la sortie\n        self.fc_out = nn.Linear(input_size, self.dim_sortie)\n        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n\n    def __repr__(self) -&gt; str:\n        return \"PolitiqueReinforce\"\n\n    def forward(self, etat: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Etat: un tenseur d'\u00e9tat (entr\u00e9e)\n        return: Distribution de probabilit\u00e9 sur les actions\n        \"\"\"\n        x = etat\n        for layer in self.couches_cachees:\n            x = F.relu(layer(x))\n        x = self.fc_out(x)\n        return F.softmax(\n            x, dim=1\n        )  # Softmax pour obtenir une distribution de probabilit\u00e9\n\n    def action(self, etat: np.ndarray) -&gt; tuple[int, torch.Tensor]:\n        \"\"\"\n        Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action.\n        etat: un \u00e9tat de l'environnement\n        return: action \u00e0 ex\u00e9cuter et la log-proba de cette action\n        \"\"\"\n        if isinstance(etat, np.ndarray):\n            etat = torch.tensor(etat, dtype=torch.float).unsqueeze(\n                0\n            )  # Ajouter une dimension pour le batch\n        proba = self.forward(etat)\n        m = Categorical(proba)\n        action = m.sample()\n        log_proba = m.log_prob(action)\n        return action.item(), log_proba\n\n    def output(self, etat: np.ndarray) -&gt; int:\n        \"\"\"\n        Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.\n        \"\"\"\n        # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n        action, _ = self.action(etat)\n        return action\n\n    def trajectoire(self, reward_fonc) -&gt; list[list, list, bool]:\n        \"\"\"\n        Simule une trajectoire dans l'environnement en utilisant la politique.\n        env: Environnement Gymnasium\n        max_t: nombre max de pas de la trajectoire\n        return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e\n        \"\"\"\n        etat, _ = self.env.reset(seed=random.randint(0, 5000))\n        recompenses = []\n        log_probas = []\n        is_success = False\n        for t in range(self.max_t):\n            action, log_proba = self.action(etat)\n            etat_suivant, recompense, fini, truncated, _ = self.env.step(action)\n            if reward_fonc is not None:\n                recompense = reward_fonc(etat_suivant, fini, truncated)\n            recompenses.append(recompense)\n            log_probas.append(log_proba)\n            if fini:\n                return recompenses, log_probas, is_success\n            if truncated:\n                is_success = True\n                return recompenses, log_probas, is_success\n            etat = etat_suivant\n\n        return recompenses, log_probas, is_success\n\n    def calcul_retours_cumules(self, recompenses: list) -&gt; list:\n        \"\"\"\n        Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma.\n        recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire\n        return: liste des retours cumul\u00e9s\n        \"\"\"\n        retour_cum = []\n        rec_cum_t = 0\n        for recompense in reversed(recompenses):\n            rec_cum_t = recompense + self.gamma * rec_cum_t\n            retour_cum.insert(0, rec_cum_t)  # Ins\u00e9rer au d\u00e9but pour maintenir l'ordre\n        return retour_cum\n\n    def loss(self, log_probs: list, retours_cumules: list) -&gt; torch.Tensor:\n        \"\"\"\n        Calcule la perte de type REINFORCE.\n        log_probs: liste des log-probas des actions prises lors d'une trajectoire\n        retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s\n        return: perte\n        \"\"\"\n        loss = [\n            -log_prob * retour for log_prob, retour in zip(log_probs, retours_cumules)\n        ]\n        return torch.cat(loss).sum()\n\n    def train(\n        self,\n        reward_func=None,\n        save_name: str = \"\",\n        stop: threading.Event | None = None,\n    ) -&gt; tuple[dict, list, float, int]:\n        \"\"\"\n        Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.\n\n        Args:\n            reward_func (callable, optional): Fonction de r\u00e9compense personnalis\u00e9e.\n            nb_episodes (int): Nombre maximum d'\u00e9pisodes.\n            max_t (int): Nombre maximum de pas par \u00e9pisode.\n            save_name (str): Chemin pour sauvegarder le mod\u00e8le.\n\n        Returns:\n            tuple: (Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).\n        \"\"\"\n        # Sauvegarde des param\u00e8tres initiaux\n        cp_policy = deepcopy(self)\n\n        # Initialisation des m\u00e9triques\n        recompenses = []\n        a_la_suite = 0\n        nb_success = 0\n        score_max = 0\n\n        for ep in range(self.nb_episodes):\n            if stop is not None:\n                if stop.is_set():\n                    break\n            self.optimizer.zero_grad()\n\n            # G\u00e9n\u00e9ration de trajectoire et calcul des r\u00e9compenses\n            recompense_ep, log_proba_ep, success = cp_policy.trajectoire(reward_func)\n            nb_success += success\n\n            # Calcul des retours cumul\u00e9s\n            retours_cum = cp_policy.calcul_retours_cumules(recompense_ep)\n            recompenses.append(sum(recompense_ep))\n\n            # Calcul et application du gradient\n            loss = cp_policy.loss(log_proba_ep, retours_cum)\n            loss.backward()\n            cp_policy.optimizer.step()\n\n            # Condition d'arr\u00eat si le score maximum est atteint plusieurs fois cons\u00e9cutivement\n            if recompenses[-1] == score_max:\n                a_la_suite += 1\n                score_max = recompenses[-1]\n            else:\n                a_la_suite = 0\n\n            if recompenses[-1] &gt; score_max:\n                score_max = recompenses[-1]\n                a_la_suite = 0\n\n            if a_la_suite == 10:\n                cp_policy.save(save_name)\n                break\n\n        # Calcul du taux de succ\u00e8s\n        taux_success = nb_success / ep\n\n        return cp_policy, recompenses, taux_success, ep + 1\n\n    def save(self, file: str):\n        \"\"\"\n        save the model.\n        \"\"\"\n        torch.save(self.state_dict(), file)\n\n    def load(self, file: str):\n        \"\"\"\n        load the model form file\n        \"\"\"\n        self.load_state_dict(torch.load(file))\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.__init__","title":"<code>__init__(env, couche_cachee, nb_episodes=2000, max_t=1000, gamma=0.99)</code>","text":"<p>env: Environnement Gymnasium couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32]) gamma: Facteur de discount pour le calcul des retours cumul\u00e9s det: Si la politique est d\u00e9terministe ou stochastique</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def __init__(\n    self,\n    env,\n    couche_cachee: list,\n    nb_episodes: int = 2000,\n    max_t: int = 1000,\n    gamma: float = 0.99,\n):\n    \"\"\"\n    env: Environnement Gymnasium\n    couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32])\n    gamma: Facteur de discount pour le calcul des retours cumul\u00e9s\n    det: Si la politique est d\u00e9terministe ou stochastique\n    \"\"\"\n    super(Reinforce, self).__init__()\n    self.dim_entree = env.observation_space.shape[0]\n    self.dim_sortie = env.action_space.n\n    self.gamma = gamma\n    self.env = env\n\n    self.nb_episodes = nb_episodes\n    self.max_t = max_t\n\n    # Cr\u00e9er dynamiquement les couches cach\u00e9es\n    self.couches_cachees = []\n    input_size = self.dim_entree\n    for hidden_size in couche_cachee:\n        self.couches_cachees.append(nn.Linear(input_size, hidden_size))\n        input_size = hidden_size\n    self.couches_cachees = nn.ModuleList(self.couches_cachees)\n\n    # La derni\u00e8re couche qui produit la sortie\n    self.fc_out = nn.Linear(input_size, self.dim_sortie)\n    self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.action","title":"<code>action(etat)</code>","text":"<p>Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action. etat: un \u00e9tat de l'environnement return: action \u00e0 ex\u00e9cuter et la log-proba de cette action</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def action(self, etat: np.ndarray) -&gt; tuple[int, torch.Tensor]:\n    \"\"\"\n    Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action.\n    etat: un \u00e9tat de l'environnement\n    return: action \u00e0 ex\u00e9cuter et la log-proba de cette action\n    \"\"\"\n    if isinstance(etat, np.ndarray):\n        etat = torch.tensor(etat, dtype=torch.float).unsqueeze(\n            0\n        )  # Ajouter une dimension pour le batch\n    proba = self.forward(etat)\n    m = Categorical(proba)\n    action = m.sample()\n    log_proba = m.log_prob(action)\n    return action.item(), log_proba\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.calcul_retours_cumules","title":"<code>calcul_retours_cumules(recompenses)</code>","text":"<p>Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma. recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire return: liste des retours cumul\u00e9s</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def calcul_retours_cumules(self, recompenses: list) -&gt; list:\n    \"\"\"\n    Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma.\n    recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire\n    return: liste des retours cumul\u00e9s\n    \"\"\"\n    retour_cum = []\n    rec_cum_t = 0\n    for recompense in reversed(recompenses):\n        rec_cum_t = recompense + self.gamma * rec_cum_t\n        retour_cum.insert(0, rec_cum_t)  # Ins\u00e9rer au d\u00e9but pour maintenir l'ordre\n    return retour_cum\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.forward","title":"<code>forward(etat)</code>","text":"<p>Etat: un tenseur d'\u00e9tat (entr\u00e9e) return: Distribution de probabilit\u00e9 sur les actions</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def forward(self, etat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Etat: un tenseur d'\u00e9tat (entr\u00e9e)\n    return: Distribution de probabilit\u00e9 sur les actions\n    \"\"\"\n    x = etat\n    for layer in self.couches_cachees:\n        x = F.relu(layer(x))\n    x = self.fc_out(x)\n    return F.softmax(\n        x, dim=1\n    )  # Softmax pour obtenir une distribution de probabilit\u00e9\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.load","title":"<code>load(file)</code>","text":"<p>load the model form file</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def load(self, file: str):\n    \"\"\"\n    load the model form file\n    \"\"\"\n    self.load_state_dict(torch.load(file))\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.loss","title":"<code>loss(log_probs, retours_cumules)</code>","text":"<p>Calcule la perte de type REINFORCE. log_probs: liste des log-probas des actions prises lors d'une trajectoire retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s return: perte</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def loss(self, log_probs: list, retours_cumules: list) -&gt; torch.Tensor:\n    \"\"\"\n    Calcule la perte de type REINFORCE.\n    log_probs: liste des log-probas des actions prises lors d'une trajectoire\n    retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s\n    return: perte\n    \"\"\"\n    loss = [\n        -log_prob * retour for log_prob, retour in zip(log_probs, retours_cumules)\n    ]\n    return torch.cat(loss).sum()\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.output","title":"<code>output(etat)</code>","text":"<p>Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def output(self, etat: np.ndarray) -&gt; int:\n    \"\"\"\n    Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.\n    \"\"\"\n    # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n    action, _ = self.action(etat)\n    return action\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.save","title":"<code>save(file)</code>","text":"<p>save the model.</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def save(self, file: str):\n    \"\"\"\n    save the model.\n    \"\"\"\n    torch.save(self.state_dict(), file)\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.train","title":"<code>train(reward_func=None, save_name='', stop=None)</code>","text":"<p>Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.</p> <p>Parameters:</p> Name Type Description Default <code>reward_func</code> <code>callable</code> <p>Fonction de r\u00e9compense personnalis\u00e9e.</p> <code>None</code> <code>nb_episodes</code> <code>int</code> <p>Nombre maximum d'\u00e9pisodes.</p> required <code>max_t</code> <code>int</code> <p>Nombre maximum de pas par \u00e9pisode.</p> required <code>save_name</code> <code>str</code> <p>Chemin pour sauvegarder le mod\u00e8le.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict, list, float, int]</code> <p>(Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def train(\n    self,\n    reward_func=None,\n    save_name: str = \"\",\n    stop: threading.Event | None = None,\n) -&gt; tuple[dict, list, float, int]:\n    \"\"\"\n    Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.\n\n    Args:\n        reward_func (callable, optional): Fonction de r\u00e9compense personnalis\u00e9e.\n        nb_episodes (int): Nombre maximum d'\u00e9pisodes.\n        max_t (int): Nombre maximum de pas par \u00e9pisode.\n        save_name (str): Chemin pour sauvegarder le mod\u00e8le.\n\n    Returns:\n        tuple: (Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).\n    \"\"\"\n    # Sauvegarde des param\u00e8tres initiaux\n    cp_policy = deepcopy(self)\n\n    # Initialisation des m\u00e9triques\n    recompenses = []\n    a_la_suite = 0\n    nb_success = 0\n    score_max = 0\n\n    for ep in range(self.nb_episodes):\n        if stop is not None:\n            if stop.is_set():\n                break\n        self.optimizer.zero_grad()\n\n        # G\u00e9n\u00e9ration de trajectoire et calcul des r\u00e9compenses\n        recompense_ep, log_proba_ep, success = cp_policy.trajectoire(reward_func)\n        nb_success += success\n\n        # Calcul des retours cumul\u00e9s\n        retours_cum = cp_policy.calcul_retours_cumules(recompense_ep)\n        recompenses.append(sum(recompense_ep))\n\n        # Calcul et application du gradient\n        loss = cp_policy.loss(log_proba_ep, retours_cum)\n        loss.backward()\n        cp_policy.optimizer.step()\n\n        # Condition d'arr\u00eat si le score maximum est atteint plusieurs fois cons\u00e9cutivement\n        if recompenses[-1] == score_max:\n            a_la_suite += 1\n            score_max = recompenses[-1]\n        else:\n            a_la_suite = 0\n\n        if recompenses[-1] &gt; score_max:\n            score_max = recompenses[-1]\n            a_la_suite = 0\n\n        if a_la_suite == 10:\n            cp_policy.save(save_name)\n            break\n\n    # Calcul du taux de succ\u00e8s\n    taux_success = nb_success / ep\n\n    return cp_policy, recompenses, taux_success, ep + 1\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.trajectoire","title":"<code>trajectoire(reward_fonc)</code>","text":"<p>Simule une trajectoire dans l'environnement en utilisant la politique. env: Environnement Gymnasium max_t: nombre max de pas de la trajectoire return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def trajectoire(self, reward_fonc) -&gt; list[list, list, bool]:\n    \"\"\"\n    Simule une trajectoire dans l'environnement en utilisant la politique.\n    env: Environnement Gymnasium\n    max_t: nombre max de pas de la trajectoire\n    return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e\n    \"\"\"\n    etat, _ = self.env.reset(seed=random.randint(0, 5000))\n    recompenses = []\n    log_probas = []\n    is_success = False\n    for t in range(self.max_t):\n        action, log_proba = self.action(etat)\n        etat_suivant, recompense, fini, truncated, _ = self.env.step(action)\n        if reward_fonc is not None:\n            recompense = reward_fonc(etat_suivant, fini, truncated)\n        recompenses.append(recompense)\n        log_probas.append(log_proba)\n        if fini:\n            return recompenses, log_probas, is_success\n        if truncated:\n            is_success = True\n            return recompenses, log_probas, is_success\n        etat = etat_suivant\n\n    return recompenses, log_probas, is_success\n</code></pre>"}]}