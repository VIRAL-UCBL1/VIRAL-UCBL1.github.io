{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to VIRAL","text":""},{"location":"#viral","title":"VIRAL","text":"<p>Vision-grounded Integration for Reward design And Learning</p>"},{"location":"setup/","title":"Welcome to VIRAL","text":""},{"location":"setup/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11</li> <li>Ollama.</li> </ul>"},{"location":"setup/#installation","title":"Installation","text":"<ul> <li>Python requirements:</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ollama:</li> </ul> <pre><code>ollama run qwen2.5-coder\n</code></pre>"},{"location":"setup/#usage","title":"Usage","text":"<pre><code>cd src\npython main.py\n</code></pre> <p>You can see help message by running:</p> <pre><code>python main.py -h\n</code></pre>"},{"location":"code_docs/ObjectivesMetrics/","title":"ObjectivesMetrics","text":""},{"location":"code_docs/ObjectivesMetrics/#src.ObjectivesMetrics.objective_metric_CartPole","title":"<code>objective_metric_CartPole(states)</code>","text":"<p>Objective metric for the CartPole environment. Calculates a score for the given state on a particular observation of the CartPole environment.</p> <p>:param state: The state of the CartPole environment. :return: a table of tuples containing the string name of the metric and the value of the metric.</p> Source code in <code>src/ObjectivesMetrics.py</code> <pre><code>def objective_metric_CartPole(states):\n    \"\"\"\n    Objective metric for the CartPole environment.\n    Calculates a score for the given state on a particular observation of the CartPole environment.\n\n    :param state: The state of the CartPole environment.\n    :return: a table of tuples containing the string name of the metric and the value of the metric.\n    \"\"\"\n\n    # Calculate the difference between the pole angle and the median of the pole angle range\n    pole_angle_diff = 0\n    for state in states:\n        pole_angle = state[2]\n        pole_angle_diff += abs(pole_angle)\n    pole_angle_diff = pole_angle_diff / len(states)\n\n    # Calculate the difference between the pole position and the median of the pole position range\n    pole_position_diff = 0\n    for state in states:\n        pole_position = state[0]\n        pole_position_diff += abs(pole_position)\n    pole_position_diff = pole_position_diff / len(states)\n\n    result = [\n        {\"pole_angle_diff\": pole_angle_diff},\n        {\"pole_position_diff\": pole_position_diff},\n    ]\n\n    return result\n</code></pre>"},{"location":"code_docs/OllamaChat/","title":"OllamaChat","text":""},{"location":"code_docs/OllamaChat/#src.OllamaChat.OllamaChat","title":"<code>OllamaChat</code>","text":"Source code in <code>src/OllamaChat.py</code> <pre><code>class OllamaChat:\n    def __init__(\n        self,\n        model: str = \"qwen2.5-coder\",\n        system_prompt: Optional[str] = None,\n        options: Optional[Dict] = None,\n    ):\n        \"\"\"\n        Initialize an advanced Ollama chat session with extended configuration.\n\n        Args:\n            model (str, optional): The name of the Ollama model.\n            system_prompt (str, optional): Initial system message to set chat context.\n            options (dict, optional): Advanced model generation parameters.\n        \"\"\"\n        self.model = model\n        self.messages: List[Dict[str, str]] = []\n        self.options = options or {}\n\n        self.logger = getLogger(\"VIRAL\")\n\n        if system_prompt:\n            self.logger.info(f\"System: {system_prompt}\")\n            self.add_message(system_prompt, role=\"system\")\n\n    def add_message(self, content: str, role: str = \"user\", **kwargs) -&gt; None:\n        \"\"\"\n        Add a message to the chat history with optional metadata.\n\n        Args:\n            content (str): The message content\n            role (str, optional): Message role (user/assistant/system)\n        \"\"\"\n        message = {\"role\": role, \"content\": content, **kwargs}\n        self.messages.append(message)\n\n    def generate_response(\n        self, stream: bool = False, additional_options: Optional[Dict] = None\n    ) -&gt; Union[str, Generator]:\n        \"\"\"\n        Generate a response with advanced configuration options.\n\n        Args:\n            stream (bool, optional): Stream response in real-time\n            additional_options (dict, optional): Temporary generation options\n\n        Returns:\n            Response as string or streaming generator\n        \"\"\"\n        generation_options = {**self.options, **(additional_options or {})}\n\n        payload = {\n            \"model\": self.model,\n            \"messages\": self.messages,\n            \"stream\": stream,\n            \"options\": generation_options,\n        }\n\n        try:\n            response = requests.post(OLLAMA_CHAT_API_URL, json=payload, stream=stream)\n\n            response.raise_for_status()\n            if not stream:\n                full_response = response.json()\n                assistant_response = full_response.get(\"message\", {}).get(\"content\", \"\")\n                self.add_message(assistant_response, role=\"assistant\")\n                return assistant_response\n\n            def stream_response():\n                full_response = \"\"\n                for line in response.iter_lines():\n                    if line:\n                        try:\n                            json_response = json.loads(line.decode(\"utf-8\"))\n                            if \"message\" in json_response:\n                                chunk = json_response[\"message\"].get(\"content\", \"\")\n                                full_response += chunk\n                                yield chunk\n                        except json.JSONDecodeError:\n                            continue\n\n                if full_response:\n                    self.add_message(full_response, role=\"assistant\")\n\n            return stream_response()\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Connection error: {e}\")\n            return \"\"\n\n    def print_Generator_and_return(self, response: Generator | str, number: int = 1):\n        \"\"\"\n        Print the response if it's a generator\n        Args:\n            response (Generator | str): the response to print\n        Returns:\n            - the response formalized if is was a generator, the response itself otherwise.\n        \"\"\"\n        self.logger.debug(f\"Response {number}:\")\n        if isinstance(response, Generator):\n            response_gen = response\n            response = \"\"\n            for chunk in response_gen:\n                print(chunk, end=\"\", flush=True)\n                response += chunk\n        return response\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.OllamaChat.OllamaChat.__init__","title":"<code>__init__(model='qwen2.5-coder', system_prompt=None, options=None)</code>","text":"<p>Initialize an advanced Ollama chat session with extended configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the Ollama model.</p> <code>'qwen2.5-coder'</code> <code>system_prompt</code> <code>str</code> <p>Initial system message to set chat context.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Advanced model generation parameters.</p> <code>None</code> Source code in <code>src/OllamaChat.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"qwen2.5-coder\",\n    system_prompt: Optional[str] = None,\n    options: Optional[Dict] = None,\n):\n    \"\"\"\n    Initialize an advanced Ollama chat session with extended configuration.\n\n    Args:\n        model (str, optional): The name of the Ollama model.\n        system_prompt (str, optional): Initial system message to set chat context.\n        options (dict, optional): Advanced model generation parameters.\n    \"\"\"\n    self.model = model\n    self.messages: List[Dict[str, str]] = []\n    self.options = options or {}\n\n    self.logger = getLogger(\"VIRAL\")\n\n    if system_prompt:\n        self.logger.info(f\"System: {system_prompt}\")\n        self.add_message(system_prompt, role=\"system\")\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.OllamaChat.OllamaChat.add_message","title":"<code>add_message(content, role='user', **kwargs)</code>","text":"<p>Add a message to the chat history with optional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content</p> required <code>role</code> <code>str</code> <p>Message role (user/assistant/system)</p> <code>'user'</code> Source code in <code>src/OllamaChat.py</code> <pre><code>def add_message(self, content: str, role: str = \"user\", **kwargs) -&gt; None:\n    \"\"\"\n    Add a message to the chat history with optional metadata.\n\n    Args:\n        content (str): The message content\n        role (str, optional): Message role (user/assistant/system)\n    \"\"\"\n    message = {\"role\": role, \"content\": content, **kwargs}\n    self.messages.append(message)\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.OllamaChat.OllamaChat.generate_response","title":"<code>generate_response(stream=False, additional_options=None)</code>","text":"<p>Generate a response with advanced configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>Stream response in real-time</p> <code>False</code> <code>additional_options</code> <code>dict</code> <p>Temporary generation options</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, Generator]</code> <p>Response as string or streaming generator</p> Source code in <code>src/OllamaChat.py</code> <pre><code>def generate_response(\n    self, stream: bool = False, additional_options: Optional[Dict] = None\n) -&gt; Union[str, Generator]:\n    \"\"\"\n    Generate a response with advanced configuration options.\n\n    Args:\n        stream (bool, optional): Stream response in real-time\n        additional_options (dict, optional): Temporary generation options\n\n    Returns:\n        Response as string or streaming generator\n    \"\"\"\n    generation_options = {**self.options, **(additional_options or {})}\n\n    payload = {\n        \"model\": self.model,\n        \"messages\": self.messages,\n        \"stream\": stream,\n        \"options\": generation_options,\n    }\n\n    try:\n        response = requests.post(OLLAMA_CHAT_API_URL, json=payload, stream=stream)\n\n        response.raise_for_status()\n        if not stream:\n            full_response = response.json()\n            assistant_response = full_response.get(\"message\", {}).get(\"content\", \"\")\n            self.add_message(assistant_response, role=\"assistant\")\n            return assistant_response\n\n        def stream_response():\n            full_response = \"\"\n            for line in response.iter_lines():\n                if line:\n                    try:\n                        json_response = json.loads(line.decode(\"utf-8\"))\n                        if \"message\" in json_response:\n                            chunk = json_response[\"message\"].get(\"content\", \"\")\n                            full_response += chunk\n                            yield chunk\n                    except json.JSONDecodeError:\n                        continue\n\n            if full_response:\n                self.add_message(full_response, role=\"assistant\")\n\n        return stream_response()\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Connection error: {e}\")\n        return \"\"\n</code></pre>"},{"location":"code_docs/OllamaChat/#src.OllamaChat.OllamaChat.print_Generator_and_return","title":"<code>print_Generator_and_return(response, number=1)</code>","text":"<p>Print the response if it's a generator Args:     response (Generator | str): the response to print Returns:     - the response formalized if is was a generator, the response itself otherwise.</p> Source code in <code>src/OllamaChat.py</code> <pre><code>def print_Generator_and_return(self, response: Generator | str, number: int = 1):\n    \"\"\"\n    Print the response if it's a generator\n    Args:\n        response (Generator | str): the response to print\n    Returns:\n        - the response formalized if is was a generator, the response itself otherwise.\n    \"\"\"\n    self.logger.debug(f\"Response {number}:\")\n    if isinstance(response, Generator):\n        response_gen = response\n        response = \"\"\n        for chunk in response_gen:\n            print(chunk, end=\"\", flush=True)\n            response += chunk\n    return response\n</code></pre>"},{"location":"code_docs/State/","title":"State","text":""},{"location":"code_docs/VIRAL/","title":"Overview of VIRAL","text":""},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL","title":"<code>VIRAL</code>","text":"Source code in <code>src/VIRAL.py</code> <pre><code>class VIRAL:\n    def __init__(\n        self,\n        learning_method: Callable,\n        env,\n        objectives_metrics: List[callable] = [],\n        model: str = \"qwen2.5-coder\",\n        options: dict = {},\n    ):\n        \"\"\"\n        Initialize VIRAL architecture for dynamic reward function generation\n            Args:\n                model (str): Language model for reward generation\n                learning_method (str): Reinforcement learning method\n        \"\"\"\n        self.llm = OllamaChat(\n            model=model,\n            system_prompt=\"\"\"\n        You are an expert in Reinforcement Learning specialized in designing reward functions.\n        Strict criteria:\n        - Complete ONLY the reward function code\n        - Use Python format\n        - Give no additional explanations\n        - Focus on the Gymnasium environment \n        - Take into the observation of the state, the terminated and truncated boolean\n        - STOP immediately your completion after the last return\n        \"\"\",\n            options=options,\n        )\n        self.env = env\n        self.objectives_metrics = objectives_metrics\n        self.learning_method = learning_method\n        self.memory: List[State] = [State(0)]\n        self.logger = getLogger(\"VIRAL\")\n        self.stops_threads = threading.Event()\n        self.lock = threading.Lock()\n        self.threads: list[threading.Thread] = []\n        self.threads.append(\n            threading.Thread(target=self._learning, args=[self.memory[0]])\n        )\n        self.threads[0].start()\n        signal.signal(signal.SIGTERM, self.sigterm_handler)\n        signal.signal(signal.SIGINT, self.sigterm_handler)\n\n    def sigterm_handler(self, signal, frame):\n        self.stops_threads.set()\n        for thread in self.threads:\n            if thread.is_alive():\n                thread.join()\n        if len(threading.enumerate()) &gt; 1:\n            for thread in threading.enumerate():\n                self.logger.error(f\"{thread.name},({thread.ident}) is alive\")\n        else:\n            self.logger.debug(\"end of main thread\")\n        sys.exit(0)\n\n    def generate_reward_function(\n        self, task_description: str, iterations: int = 1\n    ) -&gt; List[State]:\n        \"\"\"\n        Generate reward function using LLM\n\n        Args:\n            task_description (str): Detailed description of the task\n            environment_type (str): Type of environment (2D/3D, robotics, etc.)\n\n        Returns:\n            Callable: Generated reward function\n        \"\"\"\n        # TODO a regarder de plus pres\n        additional_options = {\n            \"temperature\": 1,\n        }\n        ### INIT STAGE ###\n        for i in [1, 2]:\n            prompt = f\"\"\"\n        Complete the reward function for a {self.env.spec.name} environment.\n        Task Description: {task_description} Iteration {i+1}/{2}\n\n        complete this sentence:\n        def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -&gt; float:\n            \\\"\\\"\\\"Reward function for {self.env.spec.name}\n\n            Args:\n                observations (np.ndarray): observation on the current state\n                terminated (bool): episode is terminated due a failure\n                truncated (bool): episode is truncated due a success\n\n            Returns:\n                float: The reward for the current step\n            \\\"\\\"\\\"\n        \"\"\"\n            self.llm.add_message(prompt)\n            response = self.llm.generate_response(\n                stream=True, additional_options=additional_options\n            )\n            response = self.llm.print_Generator_and_return(response, i)\n            reward_func, response = self._get_runnable_function(response)\n            self.memory.append(State(i, reward_func, response))\n        best_idx, worst_idx = self.evaluate_policy(1, 2)\n        self.logger.debug(f\"state to refine: {worst_idx}\")\n        new_idx = self.self_refine_reward(worst_idx)\n        ### SECOND STAGE ###\n        for n in range(iterations - 1):\n            best_idx, worst_idx = self.evaluate_policy(best_idx, new_idx)\n            self.logger.debug(f\"state to refine: {worst_idx}\")\n            new_idx = self.self_refine_reward(worst_idx)\n        return self.memory\n\n    def _get_code(self, response: str) -&gt; str:\n        cleaned_response = response.strip(\"```\").replace(\"python\", \"\").strip()\n        if \"def \" not in cleaned_response:\n            raise ValueError(\"The answer does not contain a valid function definition.\")\n        self.logger.debug(\"Code nettoy\u00e9 pour compilation :\\n\" + cleaned_response)\n        return cleaned_response\n\n    def _get_runnable_function(self, response: str, error: str = None) -&gt; Callable:\n        if error is not None:\n            self.llm.add_message(error)\n            response = self.llm.generate_response(stream=True)\n            response = self.llm.print_Generator_and_return(response)\n        try:\n            response = self._get_code(response)\n            reward_func = self._compile_reward_function(response)\n            state, _ = self.env.reset()\n            action = self.learning_method.output(state)\n            next_observation, _, terminated, truncated, _ = self.env.step(action)\n            self._test_reward_function(\n                reward_func,\n                observations=next_observation,\n                terminated=terminated,\n                truncated=truncated,\n            )\n        except ValueError as e:\n            self.logger.warning(str(e))\n            return self._get_runnable_function(response, str(e))\n        except SyntaxError as e:\n            self.logger.warning(f\"Error syntax {e}\")\n            return self._get_runnable_function(response, str(e))\n        except RuntimeError as e:\n            self.logger.warning(f\"Error execution {e}\")\n            return self._get_runnable_function(response, str(e))\n\n        return reward_func, response\n\n    def _compile_reward_function(self, response: str) -&gt; Callable:\n        \"\"\"\n        Compile the reward function from the LLM response.\n        Args:\n            response (str): LLM generated reward function.\n\n        Returns:\n            Callable: Compiled reward function.\n        \"\"\"\n\n        exec_globals = {}\n        exec_globals[\"np\"] = np\n        try:\n            exec(response, exec_globals)\n        except SyntaxError as e:\n            raise SyntaxError(f\"Syntax error in the generated code : {e}\")\n\n        reward_function_name = response.split(\"(\")[0].split()[\n            -1\n        ]  # r\u00e9cup le nom de la fonction\n        reward_function = exec_globals.get(reward_function_name)\n\n        return reward_function\n\n    def _test_reward_function(self, reward_function: Callable, *args, **kwargs):\n        \"\"\"\n        Test the compiled reward function with example inputs.\n\n        Args:\n            reward_function (Callable): The reward function to test.\n            *args: Positional arguments for the reward function.\n            **kwargs: Keyword arguments for the reward function.\n        \"\"\"\n        try:\n            reward = reward_function(*args, **kwargs)\n            self.logger.debug(f\"Reward function output: {reward}\")\n        except Exception as e:\n            raise RuntimeError(f\"Error during reward function execution: {e}\")\n\n    def self_refine_reward(self, idx: int) -&gt; Callable:\n        \"\"\"\n        Self-refinement of reward function based on performance\n\n        Args:\n            current_reward_func (Callable): Current reward function\n            performance_metrics (Dict): Performance evaluation metrics\n\n        Returns:\n            Callable: Refined reward function\n        \"\"\"\n        refinement_prompt = f\"\"\"\n        improve the reward function to:\n        - Increase success rate\n        - Optimize reward signal\n        - Maintain task objectives\n\n        your best reward function:\n        {self.memory[idx].reward_func_str}\n\n        performance:\n        {self.memory[idx].performances}\n        \"\"\"\n\n        self.llm.add_message(refinement_prompt)\n        refined_response = self.llm.generate_response(stream=True)\n        refined_response = self.llm.print_Generator_and_return(refined_response)\n        reward_func, refined_response = self._get_runnable_function(refined_response)\n        self.memory.append(State(len(self.memory), reward_func, refined_response))\n\n        return len(self.memory) - 1\n\n    def _learning(self, state: State) -&gt; None:\n        \"\"\"train a policy on an environment\"\"\"\n        self.logger.debug(f\"state {state.idx} begin is learning\")\n\n        policy, perfs, sr, nb_ep = self.learning_method.train(\n            reward_func=state.reward_func,\n            save_name=f\"model/{self.learning_method}_{self.env.spec.name}{state.idx}.model\",\n            stop=self.stops_threads,\n        )\n        self.memory[state.idx].set_policy(policy)\n        observations, rewards, sr_test = self.test_policy(policy)\n        perso_observations = []\n        for objective_metric in self.objectives_metrics:\n            perso_observations.append(objective_metric(observations))\n        self.memory[state.idx].set_performances(\n            {\n                \"train_success_rate\": sr,\n                \"train_episodes\": nb_ep,\n                \"test_success_rate\": sr_test,\n                \"test_rewards\": rewards,\n                \"custom_metrics\": perso_observations,\n            }\n        )  # TODO maybe add in to chat this state\n        self.logger.debug(f\"state {state.idx} as finished is learning\")\n\n    def evaluate_policy(self, idx1: int, idx2: int) -&gt; int:\n        \"\"\"\n        Evaluate policy performance for multiple reward functions\n\n        Args:\n            objectives_metrics (List[callable]): Custom objective metrics\n            num_episodes (int): Number of evaluation episodes\n\n        Returns:\n            Dict: Performance metrics for multiple reward functions\n        \"\"\"\n        if len(self.memory) &lt; 2:\n            self.logger.error(\"At least two reward functions are required.\")\n        to_join: int = []\n        for i in [idx1, idx2]:\n            if self.memory[i].performances is None:\n                self.threads.append(\n                    threading.Thread(target=self._learning, args=[self.memory[i]])\n                )\n                self.threads[-1].start()\n                to_join.append(i)\n        for t in to_join:\n            self.threads[t].join()\n        # TODO comparaison sur le success rate pour l'instant\n        if (\n            self.memory[idx1].performances[\"test_success_rate\"]\n            &gt; self.memory[idx2].performances[\"test_success_rate\"]\n        ):\n            return idx1, idx2\n        else:\n            return idx2, idx1\n\n    def test_policy(\n        self,\n        policy,\n        reward_func=None,\n        nb_episodes=100,\n        max_t=1000,\n    ) -&gt; list:\n        all_rewards = []\n        all_states = []\n        nb_success = 0\n        x_max = 0\n        x_min = 0  # avoid div by 0\n        for epi in range(1, nb_episodes + 1):\n            if self.stops_threads.is_set():\n                break\n            total_reward = 0\n            state, _ = self.env.reset()\n            for i in range(1, max_t + 1):\n                action = policy.output(state)\n                next_observation, reward, terminated, truncated, _ = self.env.step(\n                    action\n                )\n                if reward_func is not None:\n                    reward = reward_func(next_observation, terminated, truncated)\n                total_reward += reward\n                state = next_observation\n                all_states.append(state)\n                if terminated:\n                    break\n                if truncated:\n                    nb_success += 1\n                    break\n            all_rewards.append(total_reward)\n            if total_reward &gt; x_max:\n                x_max = total_reward\n            if total_reward &lt; x_min:\n                x_min = total_reward\n        all_rewards = [\n            x - x_min / x_max - x_min for x in all_rewards\n        ]  # Min-Max normalized\n        return all_states, all_rewards, (nb_success / nb_episodes)\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.__init__","title":"<code>__init__(learning_method, env, objectives_metrics=[], model='qwen2.5-coder', options={})</code>","text":"<p>Initialize VIRAL architecture for dynamic reward function generation     Args:         model (str): Language model for reward generation         learning_method (str): Reinforcement learning method</p> Source code in <code>src/VIRAL.py</code> <pre><code>def __init__(\n    self,\n    learning_method: Callable,\n    env,\n    objectives_metrics: List[callable] = [],\n    model: str = \"qwen2.5-coder\",\n    options: dict = {},\n):\n    \"\"\"\n    Initialize VIRAL architecture for dynamic reward function generation\n        Args:\n            model (str): Language model for reward generation\n            learning_method (str): Reinforcement learning method\n    \"\"\"\n    self.llm = OllamaChat(\n        model=model,\n        system_prompt=\"\"\"\n    You are an expert in Reinforcement Learning specialized in designing reward functions.\n    Strict criteria:\n    - Complete ONLY the reward function code\n    - Use Python format\n    - Give no additional explanations\n    - Focus on the Gymnasium environment \n    - Take into the observation of the state, the terminated and truncated boolean\n    - STOP immediately your completion after the last return\n    \"\"\",\n        options=options,\n    )\n    self.env = env\n    self.objectives_metrics = objectives_metrics\n    self.learning_method = learning_method\n    self.memory: List[State] = [State(0)]\n    self.logger = getLogger(\"VIRAL\")\n    self.stops_threads = threading.Event()\n    self.lock = threading.Lock()\n    self.threads: list[threading.Thread] = []\n    self.threads.append(\n        threading.Thread(target=self._learning, args=[self.memory[0]])\n    )\n    self.threads[0].start()\n    signal.signal(signal.SIGTERM, self.sigterm_handler)\n    signal.signal(signal.SIGINT, self.sigterm_handler)\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.evaluate_policy","title":"<code>evaluate_policy(idx1, idx2)</code>","text":"<p>Evaluate policy performance for multiple reward functions</p> <p>Parameters:</p> Name Type Description Default <code>objectives_metrics</code> <code>List[callable]</code> <p>Custom objective metrics</p> required <code>num_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>int</code> <p>Performance metrics for multiple reward functions</p> Source code in <code>src/VIRAL.py</code> <pre><code>def evaluate_policy(self, idx1: int, idx2: int) -&gt; int:\n    \"\"\"\n    Evaluate policy performance for multiple reward functions\n\n    Args:\n        objectives_metrics (List[callable]): Custom objective metrics\n        num_episodes (int): Number of evaluation episodes\n\n    Returns:\n        Dict: Performance metrics for multiple reward functions\n    \"\"\"\n    if len(self.memory) &lt; 2:\n        self.logger.error(\"At least two reward functions are required.\")\n    to_join: int = []\n    for i in [idx1, idx2]:\n        if self.memory[i].performances is None:\n            self.threads.append(\n                threading.Thread(target=self._learning, args=[self.memory[i]])\n            )\n            self.threads[-1].start()\n            to_join.append(i)\n    for t in to_join:\n        self.threads[t].join()\n    # TODO comparaison sur le success rate pour l'instant\n    if (\n        self.memory[idx1].performances[\"test_success_rate\"]\n        &gt; self.memory[idx2].performances[\"test_success_rate\"]\n    ):\n        return idx1, idx2\n    else:\n        return idx2, idx1\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.generate_reward_function","title":"<code>generate_reward_function(task_description, iterations=1)</code>","text":"<p>Generate reward function using LLM</p> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>Detailed description of the task</p> required <code>environment_type</code> <code>str</code> <p>Type of environment (2D/3D, robotics, etc.)</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>List[State]</code> <p>Generated reward function</p> Source code in <code>src/VIRAL.py</code> <pre><code>def generate_reward_function(\n    self, task_description: str, iterations: int = 1\n) -&gt; List[State]:\n    \"\"\"\n    Generate reward function using LLM\n\n    Args:\n        task_description (str): Detailed description of the task\n        environment_type (str): Type of environment (2D/3D, robotics, etc.)\n\n    Returns:\n        Callable: Generated reward function\n    \"\"\"\n    # TODO a regarder de plus pres\n    additional_options = {\n        \"temperature\": 1,\n    }\n    ### INIT STAGE ###\n    for i in [1, 2]:\n        prompt = f\"\"\"\n    Complete the reward function for a {self.env.spec.name} environment.\n    Task Description: {task_description} Iteration {i+1}/{2}\n\n    complete this sentence:\n    def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -&gt; float:\n        \\\"\\\"\\\"Reward function for {self.env.spec.name}\n\n        Args:\n            observations (np.ndarray): observation on the current state\n            terminated (bool): episode is terminated due a failure\n            truncated (bool): episode is truncated due a success\n\n        Returns:\n            float: The reward for the current step\n        \\\"\\\"\\\"\n    \"\"\"\n        self.llm.add_message(prompt)\n        response = self.llm.generate_response(\n            stream=True, additional_options=additional_options\n        )\n        response = self.llm.print_Generator_and_return(response, i)\n        reward_func, response = self._get_runnable_function(response)\n        self.memory.append(State(i, reward_func, response))\n    best_idx, worst_idx = self.evaluate_policy(1, 2)\n    self.logger.debug(f\"state to refine: {worst_idx}\")\n    new_idx = self.self_refine_reward(worst_idx)\n    ### SECOND STAGE ###\n    for n in range(iterations - 1):\n        best_idx, worst_idx = self.evaluate_policy(best_idx, new_idx)\n        self.logger.debug(f\"state to refine: {worst_idx}\")\n        new_idx = self.self_refine_reward(worst_idx)\n    return self.memory\n</code></pre>"},{"location":"code_docs/VIRAL/#src.VIRAL.VIRAL.self_refine_reward","title":"<code>self_refine_reward(idx)</code>","text":"<p>Self-refinement of reward function based on performance</p> <p>Parameters:</p> Name Type Description Default <code>current_reward_func</code> <code>Callable</code> <p>Current reward function</p> required <code>performance_metrics</code> <code>Dict</code> <p>Performance evaluation metrics</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Refined reward function</p> Source code in <code>src/VIRAL.py</code> <pre><code>def self_refine_reward(self, idx: int) -&gt; Callable:\n    \"\"\"\n    Self-refinement of reward function based on performance\n\n    Args:\n        current_reward_func (Callable): Current reward function\n        performance_metrics (Dict): Performance evaluation metrics\n\n    Returns:\n        Callable: Refined reward function\n    \"\"\"\n    refinement_prompt = f\"\"\"\n    improve the reward function to:\n    - Increase success rate\n    - Optimize reward signal\n    - Maintain task objectives\n\n    your best reward function:\n    {self.memory[idx].reward_func_str}\n\n    performance:\n    {self.memory[idx].performances}\n    \"\"\"\n\n    self.llm.add_message(refinement_prompt)\n    refined_response = self.llm.generate_response(stream=True)\n    refined_response = self.llm.print_Generator_and_return(refined_response)\n    reward_func, refined_response = self._get_runnable_function(refined_response)\n    self.memory.append(State(len(self.memory), reward_func, refined_response))\n\n    return len(self.memory) - 1\n</code></pre>"},{"location":"code_docs/main/","title":"Main","text":""},{"location":"code_docs/utils/","title":"Utils","text":""},{"location":"code_docs/utils/#src.utils.plot_sumrwdperepi","title":"<code>plot_sumrwdperepi(sum_rewards)</code>","text":"<p>trace courbe de somme des rec par episodes</p> Source code in <code>src/utils.py</code> <pre><code>def plot_sumrwdperepi(sum_rewards: list):\n    \"trace courbe de somme des rec par episodes\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.plot(np.arange(len(sum_rewards)), sum_rewards)\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Episode #\")\n    plt.show()\n</code></pre>"},{"location":"code_docs/utils/#src.utils.plot_sumrwdperepi_movingavg","title":"<code>plot_sumrwdperepi_movingavg(sum_rewards, avgs)</code>","text":"<p>trace courbe de somme des rec (sum_rewards) et moyenne glissante (avgs) par episodes</p> Source code in <code>src/utils.py</code> <pre><code>def plot_sumrwdperepi_movingavg(sum_rewards: list, avgs: list):\n    \"trace courbe de somme des rec (sum_rewards) et moyenne glissante (avgs) par episodes\"\n    print(\"sum_rwd:\", type(sum_rewards))\n    print(\"avgs:\", type(avgs))\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.plot(np.arange(len(sum_rewards)), sum_rewards, label=\"sum_rwd\")\n    plt.plot(np.arange(len(avgs)), avgs, c=\"r\", label=\"average\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Episode #\")\n    plt.legend(loc=\"upper left\")\n    plt.show()\n</code></pre>"},{"location":"code_docs/utils/#src.utils.plot_sumrwdperepi_overseed2","title":"<code>plot_sumrwdperepi_overseed2(rewards_over_seeds)</code>","text":"<p>trace courbe de somme des rec par episodes moyenne + std sur plusieurs seeds</p> Source code in <code>src/utils.py</code> <pre><code>def plot_sumrwdperepi_overseed2(rewards_over_seeds: list):\n    \"\"\"\n    trace courbe de somme des rec par episodes moyenne + std sur plusieurs seeds\n\n    \"\"\"\n    # rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n    df1 = pd.DataFrame(rewards_over_seeds).melt()\n    df1.rename(columns={\"variable\": \"episodes\", \"value\": \"rwd\"}, inplace=True)\n    sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n    sns.lineplot(x=\"episodes\", y=\"rwd\", data=df1, estimator=np.mean, errorbar=\"sd\").set(\n        title=\"\"\n    )\n    plt.show()\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/","title":"DirectSearch","text":""},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch","title":"<code>DirectSearch</code>","text":"Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>class DirectSearch:\n    def __init__(\n        self, env, nb_episodes: int = 2000, max_t: int = 1000, det: int = True\n    ):\n        \"\"\"\n        env: Environnement Gymnasium\n        det: Si la politique est d\u00e9terministe ou stochastique\n\n        \"\"\"\n        self.dim_entree = env.observation_space.shape[0]\n        self.dim_sortie = env.action_space.n\n        self.det = det\n        self.nb_episodes = nb_episodes\n        self.max_t = max_t\n        self.env = env\n        # Matrice entre * sortie\n        self.poids = np.random.rand(self.dim_entree, self.dim_sortie)\n\n    def __repr__(self):\n        return \"DirectSearch\"\n\n    def output(self, etat: np.ndarray) -&gt; int:\n        \"\"\"Calcul de la sortie de la politique\n        - si d\u00e9terministe : argmax\n        - si stochastique : probabilit\u00e9 de chaque action\n        \"\"\"\n        # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n        prob = torch.nn.functional.softmax(torch.tensor(etat.dot(self.poids)), dim=0)\n        if self.det:\n            return torch.argmax(prob).item()\n        else:\n            return torch.Categorical(probs=prob).sample().item()\n\n    def set_poids(self, poids: np.ndarray):\n        self.poids = poids\n\n    def get_poids(self) -&gt; np.ndarray:\n        return self.poids\n\n    def save(self, file):\n        f = open(file, \"w\")\n        f.write(f\"{self.poids};{self.det}\")\n\n    def load(self, file):\n        f = open(file, \"r\")\n        param = f.read().split(\";\")\n        self.poids = param[0]\n        self.det = param[1]\n\n    def rollout(self, reward_func) -&gt; int:\n        \"\"\"\n        execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode\n        \"\"\"\n        total_rec = 0\n        is_success = False\n        state, _ = self.env.reset()\n        for _ in range(1, self.max_t + 1):\n            action = self.output(state)\n            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n            if reward_func is not None:\n                reward = reward_func(next_observation, terminated, truncated)\n            total_rec += reward\n            state = next_observation\n            if terminated:\n                return total_rec, is_success\n            if truncated:\n                is_success = True\n                return total_rec, is_success\n        return total_rec, is_success\n\n    def train(\n        self, reward_func=None, save_name=\"\", stop: threading.Event | None = None\n    ) -&gt; tuple[list, np.ndarray]:\n        cp_policy: DirectSearch = deepcopy(self)\n        bruit_std = 1e-2\n        meilleur_perf = 0\n        meilleur_poid = cp_policy.get_poids()\n        perf_by_episode = list()\n        nb_best_perf = 0\n        nb_success = 0\n        for i_episode in range(1, self.nb_episodes + 1):\n            if stop is not None:\n                if stop.is_set():\n                    break\n            perf, success = cp_policy.rollout(reward_func)\n            nb_success += success\n            perf_by_episode.append(perf)\n\n            if perf == meilleur_perf:\n                nb_best_perf += 1\n            else:\n                nb_best_perf = 0\n            if nb_best_perf == 10:\n                break\n            if perf &gt;= meilleur_perf:\n                meilleur_perf = perf\n                meilleur_poid = cp_policy.get_poids()\n                if perf &gt; meilleur_perf:\n                    nb_best_perf = 0\n                # reduction de la variance du bruit\n                bruit_std = max(1e-3, bruit_std / 2)\n            else:\n                # augmentation de la variance du bruit\n                bruit_std = min(2, bruit_std * 2)\n            # On calcule le bruit en fonction de la variance\n            bruit = np.random.normal(\n                0, bruit_std, cp_policy.dim_entree * cp_policy.dim_sortie\n            )\n            # Reshape le bruit pour qu'il ait la m\u00eame taille que les poids\n            bruit = bruit.reshape(cp_policy.dim_entree, cp_policy.dim_sortie)\n            # On ajoute le bruit aux poids\n            cp_policy.set_poids(meilleur_poid + bruit)\n        if save_name is not None:\n            cp_policy.save(save_name)\n        return cp_policy, perf_by_episode, (nb_success / i_episode), i_episode\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.__init__","title":"<code>__init__(env, nb_episodes=2000, max_t=1000, det=True)</code>","text":"<p>env: Environnement Gymnasium det: Si la politique est d\u00e9terministe ou stochastique</p> Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>def __init__(\n    self, env, nb_episodes: int = 2000, max_t: int = 1000, det: int = True\n):\n    \"\"\"\n    env: Environnement Gymnasium\n    det: Si la politique est d\u00e9terministe ou stochastique\n\n    \"\"\"\n    self.dim_entree = env.observation_space.shape[0]\n    self.dim_sortie = env.action_space.n\n    self.det = det\n    self.nb_episodes = nb_episodes\n    self.max_t = max_t\n    self.env = env\n    # Matrice entre * sortie\n    self.poids = np.random.rand(self.dim_entree, self.dim_sortie)\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.output","title":"<code>output(etat)</code>","text":"<p>Calcul de la sortie de la politique - si d\u00e9terministe : argmax - si stochastique : probabilit\u00e9 de chaque action</p> Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>def output(self, etat: np.ndarray) -&gt; int:\n    \"\"\"Calcul de la sortie de la politique\n    - si d\u00e9terministe : argmax\n    - si stochastique : probabilit\u00e9 de chaque action\n    \"\"\"\n    # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n    prob = torch.nn.functional.softmax(torch.tensor(etat.dot(self.poids)), dim=0)\n    if self.det:\n        return torch.argmax(prob).item()\n    else:\n        return torch.Categorical(probs=prob).sample().item()\n</code></pre>"},{"location":"code_docs/RLAlgo/DirectSearch/#src.RLAlgo.DirectSearch.DirectSearch.rollout","title":"<code>rollout(reward_func)</code>","text":"<p>execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode</p> Source code in <code>src/RLAlgo/DirectSearch.py</code> <pre><code>def rollout(self, reward_func) -&gt; int:\n    \"\"\"\n    execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'\u00e9pisode\n    \"\"\"\n    total_rec = 0\n    is_success = False\n    state, _ = self.env.reset()\n    for _ in range(1, self.max_t + 1):\n        action = self.output(state)\n        next_observation, reward, terminated, truncated, _ = self.env.step(action)\n        if reward_func is not None:\n            reward = reward_func(next_observation, terminated, truncated)\n        total_rec += reward\n        state = next_observation\n        if terminated:\n            return total_rec, is_success\n        if truncated:\n            is_success = True\n            return total_rec, is_success\n    return total_rec, is_success\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/","title":"PPO","text":""},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO","title":"<code>PPO</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>class PPO(nn.Module):\n    def __init__(\n        self,\n        env: gym.Env,\n        hidden_size: int = 256,\n        std: float = 0.0,\n        batch_size: int = 5,\n        ppo_epoch: int = 4,\n        lr: float = 3e-4,\n        nb_episodes: int = 2000,\n        max_t: int = 1000,\n    ):\n        \"\"\" \"\"\"\n        super(PPO, self).__init__()\n        self.num_inputs = env.observation_space.shape[0]\n        self.num_outputs = env.action_space.n\n        self.critic = nn.Sequential(\n            nn.Linear(self.num_inputs, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1),\n        )  # predict the reward from a state\n\n        self.actor = nn.Sequential(\n            nn.Linear(self.num_inputs, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, self.num_outputs),\n        )  # predict the action todo\n        self.log_std = nn.Parameter(\n            torch.ones(1, self.num_outputs) * std\n        )  # compute the std\n\n        def init_weights(m):\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.1)\n                nn.init.constant_(m.bias, 0.1)\n\n        self.apply(init_weights)  # help to converge\n\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.nb_episodes = nb_episodes\n        self.max_t = max_t\n        self.ppo_epoch = ppo_epoch\n        self.lr = lr\n        self.batch_size = batch_size\n        self.env = env\n\n    def forward(self, x) -&gt; tuple[Normal, float]:\n        if isinstance(x, np.ndarray):\n            x = torch.Tensor(x)\n        value = self.critic(x)  # predict the reward\n        mu = self.actor(x)  # predict the action\n        std = self.log_std.exp()  # compute std\n        dist = Normal(mu, std)  # make a normal distribution of proba\n        return dist, value\n\n    def __repr__(self) -&gt; str:\n        return \"PPO\"\n\n    def output(self, state: np.ndarray) -&gt; int:\n        \"\"\" \"\"\"\n        dist, _ = self.forward(state)\n        action = dist.sample()\n        return action.item()\n\n    def save(self, file: str):\n        \"\"\"\n        save the model.\n        \"\"\"\n        torch.save(self.state_dict(), file)\n\n    def load(self, file: str):\n        \"\"\"\n        load the model form file\n        \"\"\"\n        self.load_state_dict(torch.load(file))\n\n    def train(\n        self,\n        reward_func=None,\n        save_name: str = \"\",\n        stop: threading.Event | None = None,\n    ) -&gt; tuple[dict, list, float, int]:\n        # Sauvegarde des param\u00e8tres initiaux\n        cp_policy = deepcopy(self)\n\n        # Initialisation des m\u00e9triques\n        recompenses = []\n        a_la_suite = 0\n        nb_success = 0\n        score_max = 0\n\n        state, _ = cp_policy.env.reset()\n        for i_episode in range(cp_policy.nb_episodes):\n            if stop is not None:\n                if stop.is_set():\n                    break\n            log_probs = []\n            values = []\n            states = []\n            actions = []\n            rewards = []\n            # Run policy T times\n            for t in range(cp_policy.max_t):\n                dist, value = cp_policy.forward(state)\n                action = dist.sample()\n                log_prob = dist.log_prob(action)\n                action = action.item()\n                next_state, reward, terminated, truncated, _ = cp_policy.env.step(action)\n                log_probs.append(log_prob)\n                values.append(value)\n                actions.append(action)\n                rewards.append(reward)\n                states.append(state)\n                actions.append(action)\n                state = next_state\n                if terminated:\n                    break\n                if truncated:\n                    nb_success +=1\n                    break\n            # compute \u00c2_1 ... \u00c2_t\n            _, value = cp_policy.forward(state)\n            values.append(value)\n            advantage_estimates, returns = cp_policy._global_advantage_estimates(\n                rewards, values\n            )\n            # optimise\n            dataset = cp_policy.PPODataset(\n                states, actions, log_probs, returns, advantage_estimates\n            )\n            data_loader = DataLoader(dataset, batch_size=cp_policy.batch_size, shuffle=True)\n            cp_policy._ppo_update(data_loader)\n        if save_name is not None:\n            cp_policy.save(save_name)\n        return cp_policy, rewards, (nb_success / i_episode), i_episode\n\n    def _global_advantage_estimates(self, rewards, values, gamma=0.99, tau=0.95):\n        gae = 0\n        returns = []\n        for step in reversed(range(len(rewards))):\n            delta = rewards[step] + gamma * values[step + 1] - values[step]\n            gae = delta + gamma * tau * gae\n            returns.insert(0, gae + values[step])\n        return returns - values, returns\n\n    def _ppo_update(\n        self, dataloader: DataLoader):\n        for _ in range(self.ppo_epochs):\n            for state, action, old_log_probs, return_, advantage in dataloader:\n                dist, value = self.forward(state)\n                entropy = dist.entropy().mean()\n                new_log_probs = dist.log_prob(action)\n\n                ratio = (new_log_probs - old_log_probs).exp()\n                surr1 = ratio * advantage\n                surr2 = (\n                    torch.clamp(ratio, 0.8, 1.2) * advantage\n                )\n\n                actor_loss = -torch.min(surr1, surr2).mean()\n                critic_loss = (return_ - value).pow(2).mean()\n\n                loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n    class PPODataset(Dataset):\n        def __init__(self, states, actions, log_probs, returns, advantages):\n            self.states = states\n            self.actions = actions\n            self.log_probs = log_probs\n            self.returns = returns\n            self.advantages = advantages\n\n        def __len__(self):\n            return self.states.size(0)\n\n        def __getitem__(self, idx):\n            return (\n                self.states[idx, :],\n                self.actions[idx, :],\n                self.log_probs[idx, :],\n                self.returns[idx, :],\n                self.advantages[idx, :],\n            )\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.__init__","title":"<code>__init__(env, hidden_size=256, std=0.0, batch_size=5, ppo_epoch=4, lr=0.0003, nb_episodes=2000, max_t=1000)</code>","text":"Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def __init__(\n    self,\n    env: gym.Env,\n    hidden_size: int = 256,\n    std: float = 0.0,\n    batch_size: int = 5,\n    ppo_epoch: int = 4,\n    lr: float = 3e-4,\n    nb_episodes: int = 2000,\n    max_t: int = 1000,\n):\n    \"\"\" \"\"\"\n    super(PPO, self).__init__()\n    self.num_inputs = env.observation_space.shape[0]\n    self.num_outputs = env.action_space.n\n    self.critic = nn.Sequential(\n        nn.Linear(self.num_inputs, hidden_size),\n        nn.ReLU(),\n        nn.Linear(hidden_size, 1),\n    )  # predict the reward from a state\n\n    self.actor = nn.Sequential(\n        nn.Linear(self.num_inputs, hidden_size),\n        nn.ReLU(),\n        nn.Linear(hidden_size, self.num_outputs),\n    )  # predict the action todo\n    self.log_std = nn.Parameter(\n        torch.ones(1, self.num_outputs) * std\n    )  # compute the std\n\n    def init_weights(m):\n        if isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, mean=0.0, std=0.1)\n            nn.init.constant_(m.bias, 0.1)\n\n    self.apply(init_weights)  # help to converge\n\n    self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n    self.nb_episodes = nb_episodes\n    self.max_t = max_t\n    self.ppo_epoch = ppo_epoch\n    self.lr = lr\n    self.batch_size = batch_size\n    self.env = env\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.load","title":"<code>load(file)</code>","text":"<p>load the model form file</p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def load(self, file: str):\n    \"\"\"\n    load the model form file\n    \"\"\"\n    self.load_state_dict(torch.load(file))\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.output","title":"<code>output(state)</code>","text":"Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def output(self, state: np.ndarray) -&gt; int:\n    \"\"\" \"\"\"\n    dist, _ = self.forward(state)\n    action = dist.sample()\n    return action.item()\n</code></pre>"},{"location":"code_docs/RLAlgo/PPO/#src.RLAlgo.PPO.PPO.save","title":"<code>save(file)</code>","text":"<p>save the model.</p> Source code in <code>src/RLAlgo/PPO.py</code> <pre><code>def save(self, file: str):\n    \"\"\"\n    save the model.\n    \"\"\"\n    torch.save(self.state_dict(), file)\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/","title":"Reinforce","text":""},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce","title":"<code>Reinforce</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>class Reinforce(nn.Module):\n    def __init__(\n        self,\n        env,\n        couche_cachee: list,\n        nb_episodes: int = 2000,\n        max_t: int = 1000,\n        gamma: float = 0.99,\n    ):\n        \"\"\"\n        env: Environnement Gymnasium\n        couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32])\n        gamma: Facteur de discount pour le calcul des retours cumul\u00e9s\n        det: Si la politique est d\u00e9terministe ou stochastique\n        \"\"\"\n        super(Reinforce, self).__init__()\n        self.dim_entree = env.observation_space.shape[0]\n        self.dim_sortie = env.action_space.n\n        self.gamma = gamma\n        self.env = env\n\n        self.nb_episodes = nb_episodes\n        self.max_t = max_t\n\n        # Cr\u00e9er dynamiquement les couches cach\u00e9es\n        self.couches_cachees = []\n        input_size = self.dim_entree\n        for hidden_size in couche_cachee:\n            self.couches_cachees.append(nn.Linear(input_size, hidden_size))\n            input_size = hidden_size\n        self.couches_cachees = nn.ModuleList(self.couches_cachees)\n\n        # La derni\u00e8re couche qui produit la sortie\n        self.fc_out = nn.Linear(input_size, self.dim_sortie)\n        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n\n    def __repr__(self) -&gt; str:\n        return \"PolitiqueReinforce\"\n\n    def forward(self, etat: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Etat: un tenseur d'\u00e9tat (entr\u00e9e)\n        return: Distribution de probabilit\u00e9 sur les actions\n        \"\"\"\n        x = etat\n        for layer in self.couches_cachees:\n            x = F.relu(layer(x))\n        x = self.fc_out(x)\n        return F.softmax(\n            x, dim=1\n        )  # Softmax pour obtenir une distribution de probabilit\u00e9\n\n    def action(self, etat: np.ndarray) -&gt; tuple[int, torch.Tensor]:\n        \"\"\"\n        Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action.\n        etat: un \u00e9tat de l'environnement\n        return: action \u00e0 ex\u00e9cuter et la log-proba de cette action\n        \"\"\"\n        if isinstance(etat, np.ndarray):\n            etat = torch.tensor(etat, dtype=torch.float).unsqueeze(\n                0\n            )  # Ajouter une dimension pour le batch\n        proba = self.forward(etat)\n        m = Categorical(proba)\n        action = m.sample()\n        log_proba = m.log_prob(action)\n        return action.item(), log_proba\n\n    def output(self, etat: np.ndarray) -&gt; int:\n        \"\"\"\n        Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.\n        \"\"\"\n        # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n        action, _ = self.action(etat)\n        return action\n\n    def trajectoire(self, reward_fonc) -&gt; list[list, list, bool]:\n        \"\"\"\n        Simule une trajectoire dans l'environnement en utilisant la politique.\n        env: Environnement Gymnasium\n        max_t: nombre max de pas de la trajectoire\n        return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e\n        \"\"\"\n        etat, _ = self.env.reset(seed=random.randint(0, 5000))\n        recompenses = []\n        log_probas = []\n        is_success = False\n        for t in range(self.max_t):\n            action, log_proba = self.action(etat)\n            etat_suivant, recompense, fini, truncated, _ = self.env.step(action)\n            if reward_fonc is not None:\n                recompense = reward_fonc(etat_suivant, fini, truncated)\n            recompenses.append(recompense)\n            log_probas.append(log_proba)\n            if fini:\n                return recompenses, log_probas, is_success\n            if truncated:\n                is_success = True\n                return recompenses, log_probas, is_success\n            etat = etat_suivant\n\n        return recompenses, log_probas, is_success\n\n    def calcul_retours_cumules(self, recompenses: list) -&gt; list:\n        \"\"\"\n        Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma.\n        recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire\n        return: liste des retours cumul\u00e9s\n        \"\"\"\n        retour_cum = []\n        rec_cum_t = 0\n        for recompense in reversed(recompenses):\n            rec_cum_t = recompense + self.gamma * rec_cum_t\n            retour_cum.insert(0, rec_cum_t)  # Ins\u00e9rer au d\u00e9but pour maintenir l'ordre\n        return retour_cum\n\n    def loss(self, log_probs: list, retours_cumules: list) -&gt; torch.Tensor:\n        \"\"\"\n        Calcule la perte de type REINFORCE.\n        log_probs: liste des log-probas des actions prises lors d'une trajectoire\n        retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s\n        return: perte\n        \"\"\"\n        loss = [\n            -log_prob * retour for log_prob, retour in zip(log_probs, retours_cumules)\n        ]\n        return torch.cat(loss).sum()\n\n    def train(\n        self,\n        reward_func=None,\n        save_name: str = \"\",\n        stop: threading.Event | None = None,\n    ) -&gt; tuple[dict, list, float, int]:\n        \"\"\"\n        Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.\n\n        Args:\n            reward_func (callable, optional): Fonction de r\u00e9compense personnalis\u00e9e.\n            nb_episodes (int): Nombre maximum d'\u00e9pisodes.\n            max_t (int): Nombre maximum de pas par \u00e9pisode.\n            save_name (str): Chemin pour sauvegarder le mod\u00e8le.\n\n        Returns:\n            tuple: (Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).\n        \"\"\"\n        # Sauvegarde des param\u00e8tres initiaux\n        cp_policy = deepcopy(self)\n\n        # Initialisation des m\u00e9triques\n        recompenses = []\n        a_la_suite = 0\n        nb_success = 0\n        score_max = 0\n\n        for ep in range(self.nb_episodes):\n            if stop is not None:\n                if stop.is_set():\n                    break\n            self.optimizer.zero_grad()\n\n            # G\u00e9n\u00e9ration de trajectoire et calcul des r\u00e9compenses\n            recompense_ep, log_proba_ep, success = cp_policy.trajectoire(reward_func)\n            nb_success += success\n\n            # Calcul des retours cumul\u00e9s\n            retours_cum = cp_policy.calcul_retours_cumules(recompense_ep)\n            recompenses.append(sum(recompense_ep))\n\n            # Calcul et application du gradient\n            loss = cp_policy.loss(log_proba_ep, retours_cum)\n            loss.backward()\n            cp_policy.optimizer.step()\n\n            # Condition d'arr\u00eat si le score maximum est atteint plusieurs fois cons\u00e9cutivement\n            if recompenses[-1] == score_max:\n                a_la_suite += 1\n                score_max = recompenses[-1]\n            else:\n                a_la_suite = 0\n\n            if recompenses[-1] &gt; score_max:\n                score_max = recompenses[-1]\n                a_la_suite = 0\n\n            if a_la_suite == 10:\n                cp_policy.save(save_name)\n                break\n\n        # Calcul du taux de succ\u00e8s\n        taux_success = nb_success / ep\n\n        return cp_policy, recompenses, taux_success, ep + 1\n\n    def save(self, file: str):\n        \"\"\"\n        save the model.\n        \"\"\"\n        torch.save(self.state_dict(), file)\n\n    def load(self, file: str):\n        \"\"\"\n        load the model form file\n        \"\"\"\n        self.load_state_dict(torch.load(file))\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.__init__","title":"<code>__init__(env, couche_cachee, nb_episodes=2000, max_t=1000, gamma=0.99)</code>","text":"<p>env: Environnement Gymnasium couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32]) gamma: Facteur de discount pour le calcul des retours cumul\u00e9s det: Si la politique est d\u00e9terministe ou stochastique</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def __init__(\n    self,\n    env,\n    couche_cachee: list,\n    nb_episodes: int = 2000,\n    max_t: int = 1000,\n    gamma: float = 0.99,\n):\n    \"\"\"\n    env: Environnement Gymnasium\n    couche_cachee: Liste des tailles des couches cach\u00e9es (exemple [64, 32])\n    gamma: Facteur de discount pour le calcul des retours cumul\u00e9s\n    det: Si la politique est d\u00e9terministe ou stochastique\n    \"\"\"\n    super(Reinforce, self).__init__()\n    self.dim_entree = env.observation_space.shape[0]\n    self.dim_sortie = env.action_space.n\n    self.gamma = gamma\n    self.env = env\n\n    self.nb_episodes = nb_episodes\n    self.max_t = max_t\n\n    # Cr\u00e9er dynamiquement les couches cach\u00e9es\n    self.couches_cachees = []\n    input_size = self.dim_entree\n    for hidden_size in couche_cachee:\n        self.couches_cachees.append(nn.Linear(input_size, hidden_size))\n        input_size = hidden_size\n    self.couches_cachees = nn.ModuleList(self.couches_cachees)\n\n    # La derni\u00e8re couche qui produit la sortie\n    self.fc_out = nn.Linear(input_size, self.dim_sortie)\n    self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.action","title":"<code>action(etat)</code>","text":"<p>Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action. etat: un \u00e9tat de l'environnement return: action \u00e0 ex\u00e9cuter et la log-proba de cette action</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def action(self, etat: np.ndarray) -&gt; tuple[int, torch.Tensor]:\n    \"\"\"\n    Renvoi l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat et la log-proba de cette action.\n    etat: un \u00e9tat de l'environnement\n    return: action \u00e0 ex\u00e9cuter et la log-proba de cette action\n    \"\"\"\n    if isinstance(etat, np.ndarray):\n        etat = torch.tensor(etat, dtype=torch.float).unsqueeze(\n            0\n        )  # Ajouter une dimension pour le batch\n    proba = self.forward(etat)\n    m = Categorical(proba)\n    action = m.sample()\n    log_proba = m.log_prob(action)\n    return action.item(), log_proba\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.calcul_retours_cumules","title":"<code>calcul_retours_cumules(recompenses)</code>","text":"<p>Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma. recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire return: liste des retours cumul\u00e9s</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def calcul_retours_cumules(self, recompenses: list) -&gt; list:\n    \"\"\"\n    Calcule les retours cumul\u00e9s pond\u00e9r\u00e9s par le facteur de discount gamma.\n    recompenses: liste des r\u00e9compenses obtenues lors d'une trajectoire\n    return: liste des retours cumul\u00e9s\n    \"\"\"\n    retour_cum = []\n    rec_cum_t = 0\n    for recompense in reversed(recompenses):\n        rec_cum_t = recompense + self.gamma * rec_cum_t\n        retour_cum.insert(0, rec_cum_t)  # Ins\u00e9rer au d\u00e9but pour maintenir l'ordre\n    return retour_cum\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.forward","title":"<code>forward(etat)</code>","text":"<p>Etat: un tenseur d'\u00e9tat (entr\u00e9e) return: Distribution de probabilit\u00e9 sur les actions</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def forward(self, etat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Etat: un tenseur d'\u00e9tat (entr\u00e9e)\n    return: Distribution de probabilit\u00e9 sur les actions\n    \"\"\"\n    x = etat\n    for layer in self.couches_cachees:\n        x = F.relu(layer(x))\n    x = self.fc_out(x)\n    return F.softmax(\n        x, dim=1\n    )  # Softmax pour obtenir une distribution de probabilit\u00e9\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.load","title":"<code>load(file)</code>","text":"<p>load the model form file</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def load(self, file: str):\n    \"\"\"\n    load the model form file\n    \"\"\"\n    self.load_state_dict(torch.load(file))\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.loss","title":"<code>loss(log_probs, retours_cumules)</code>","text":"<p>Calcule la perte de type REINFORCE. log_probs: liste des log-probas des actions prises lors d'une trajectoire retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s return: perte</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def loss(self, log_probs: list, retours_cumules: list) -&gt; torch.Tensor:\n    \"\"\"\n    Calcule la perte de type REINFORCE.\n    log_probs: liste des log-probas des actions prises lors d'une trajectoire\n    retours_cumules: liste des retours cumul\u00e9s pond\u00e9r\u00e9s\n    return: perte\n    \"\"\"\n    loss = [\n        -log_prob * retour for log_prob, retour in zip(log_probs, retours_cumules)\n    ]\n    return torch.cat(loss).sum()\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.output","title":"<code>output(etat)</code>","text":"<p>Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def output(self, etat: np.ndarray) -&gt; int:\n    \"\"\"\n    Calcul l'action \u00e0 ex\u00e9cuter dans l'\u00e9tat.\n    \"\"\"\n    # Nous utilisons une fonction d'activation soft max pour que les poids soient \u00e0 la m\u00eame \u00e9chelle\n    action, _ = self.action(etat)\n    return action\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.save","title":"<code>save(file)</code>","text":"<p>save the model.</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def save(self, file: str):\n    \"\"\"\n    save the model.\n    \"\"\"\n    torch.save(self.state_dict(), file)\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.train","title":"<code>train(reward_func=None, save_name='', stop=None)</code>","text":"<p>Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.</p> <p>Parameters:</p> Name Type Description Default <code>reward_func</code> <code>callable</code> <p>Fonction de r\u00e9compense personnalis\u00e9e.</p> <code>None</code> <code>nb_episodes</code> <code>int</code> <p>Nombre maximum d'\u00e9pisodes.</p> required <code>max_t</code> <code>int</code> <p>Nombre maximum de pas par \u00e9pisode.</p> required <code>save_name</code> <code>str</code> <p>Chemin pour sauvegarder le mod\u00e8le.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict, list, float, int]</code> <p>(Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def train(\n    self,\n    reward_func=None,\n    save_name: str = \"\",\n    stop: threading.Event | None = None,\n) -&gt; tuple[dict, list, float, int]:\n    \"\"\"\n    Entra\u00eene la politique en utilisant l'algorithme REINFORCE tout en restaurant les param\u00e8tres initiaux.\n\n    Args:\n        reward_func (callable, optional): Fonction de r\u00e9compense personnalis\u00e9e.\n        nb_episodes (int): Nombre maximum d'\u00e9pisodes.\n        max_t (int): Nombre maximum de pas par \u00e9pisode.\n        save_name (str): Chemin pour sauvegarder le mod\u00e8le.\n\n    Returns:\n        tuple: (Param\u00e8tres entra\u00een\u00e9s, Historique des r\u00e9compenses, Taux de succ\u00e8s, Nombre d'\u00e9pisodes ex\u00e9cut\u00e9s).\n    \"\"\"\n    # Sauvegarde des param\u00e8tres initiaux\n    cp_policy = deepcopy(self)\n\n    # Initialisation des m\u00e9triques\n    recompenses = []\n    a_la_suite = 0\n    nb_success = 0\n    score_max = 0\n\n    for ep in range(self.nb_episodes):\n        if stop is not None:\n            if stop.is_set():\n                break\n        self.optimizer.zero_grad()\n\n        # G\u00e9n\u00e9ration de trajectoire et calcul des r\u00e9compenses\n        recompense_ep, log_proba_ep, success = cp_policy.trajectoire(reward_func)\n        nb_success += success\n\n        # Calcul des retours cumul\u00e9s\n        retours_cum = cp_policy.calcul_retours_cumules(recompense_ep)\n        recompenses.append(sum(recompense_ep))\n\n        # Calcul et application du gradient\n        loss = cp_policy.loss(log_proba_ep, retours_cum)\n        loss.backward()\n        cp_policy.optimizer.step()\n\n        # Condition d'arr\u00eat si le score maximum est atteint plusieurs fois cons\u00e9cutivement\n        if recompenses[-1] == score_max:\n            a_la_suite += 1\n            score_max = recompenses[-1]\n        else:\n            a_la_suite = 0\n\n        if recompenses[-1] &gt; score_max:\n            score_max = recompenses[-1]\n            a_la_suite = 0\n\n        if a_la_suite == 10:\n            cp_policy.save(save_name)\n            break\n\n    # Calcul du taux de succ\u00e8s\n    taux_success = nb_success / ep\n\n    return cp_policy, recompenses, taux_success, ep + 1\n</code></pre>"},{"location":"code_docs/RLAlgo/Reinforce/#src.RLAlgo.Reinforce.Reinforce.trajectoire","title":"<code>trajectoire(reward_fonc)</code>","text":"<p>Simule une trajectoire dans l'environnement en utilisant la politique. env: Environnement Gymnasium max_t: nombre max de pas de la trajectoire return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e</p> Source code in <code>src/RLAlgo/Reinforce.py</code> <pre><code>def trajectoire(self, reward_fonc) -&gt; list[list, list, bool]:\n    \"\"\"\n    Simule une trajectoire dans l'environnement en utilisant la politique.\n    env: Environnement Gymnasium\n    max_t: nombre max de pas de la trajectoire\n    return: liste des r\u00e9compenses et liste des log-probas des actions prises et si la trajectoire est tronqu\u00e9e\n    \"\"\"\n    etat, _ = self.env.reset(seed=random.randint(0, 5000))\n    recompenses = []\n    log_probas = []\n    is_success = False\n    for t in range(self.max_t):\n        action, log_proba = self.action(etat)\n        etat_suivant, recompense, fini, truncated, _ = self.env.step(action)\n        if reward_fonc is not None:\n            recompense = reward_fonc(etat_suivant, fini, truncated)\n        recompenses.append(recompense)\n        log_probas.append(log_proba)\n        if fini:\n            return recompenses, log_probas, is_success\n        if truncated:\n            is_success = True\n            return recompenses, log_probas, is_success\n        etat = etat_suivant\n\n    return recompenses, log_probas, is_success\n</code></pre>"}]}